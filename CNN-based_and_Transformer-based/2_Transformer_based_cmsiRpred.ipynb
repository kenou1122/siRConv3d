{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2134ef-063d-4834-b3e0-64ef50111ed3",
   "metadata": {},
   "source": [
    "# Transformer-based cmsiRpred\n",
    "\n",
    "2-uni_v3.3_transformer_0829-Copy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5222cb3a-461c-4dec-b5a1-9e64e6a00cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import forgi.graph.bulge_graph as fgb\n",
    "\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc5133-4d77-4218-987e-4b3333bff175",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b5eb95-43c8-4595-99e1-f64e6fa030c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20626, 165) (2568, 165) (2588, 165)\n"
     ]
    }
   ],
   "source": [
    "df_structured_encoded = pd.read_pickle('/home/ken/MyStorage/siRNA_2503/Data/df_structured_encoded_0326.pkl')\n",
    "\n",
    "df_structured_encoded_iid_trvl = df_structured_encoded[df_structured_encoded['dataset_usage']=='IID_trvl'].sample(frac=1)\n",
    "df_structured_encoded_iid_test = df_structured_encoded[df_structured_encoded['dataset_usage']=='IID_test']\n",
    "df_structured_encoded_ood_test = df_structured_encoded[df_structured_encoded['dataset_usage']=='OOD_test']\n",
    "print(df_structured_encoded_iid_trvl.shape,df_structured_encoded_iid_test.shape,df_structured_encoded_ood_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab75553a-5466-4b65-be4a-26150cf47363",
   "metadata": {},
   "source": [
    "## Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbf613f-b5fb-4c13-963d-7b1d77ecb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_map = {\n",
    "    's': 0, # stem\n",
    "    'h': 1, # hairpin loop\n",
    "    'i': 2, # interior loop\n",
    "    'm': 3, # multiloop\n",
    "    'f': 4, # fiveprime\n",
    "    't': 5, # threeprime\n",
    "    'P': 7 # no nt here (pad)\n",
    "}\n",
    "nt_map = {\n",
    "    'P': 7, # no nt here (pad)\n",
    "    'A': 0,\n",
    "    'U': 1,\n",
    "    'T': 2,\n",
    "    'G': 3,\n",
    "    'C': 4,\n",
    "    'S': 5\n",
    "}\n",
    "modi_map = {\n",
    "    'P': 7\n",
    "    # 0ï½ž6 see vocab.csv\n",
    "}\n",
    "\n",
    "\n",
    "def get_nt_strtype_vec(dot_bracket,SEQ_MAX_LEN=28):\n",
    "    import forgi.graph.bulge_graph as fgb\n",
    "    bg = fgb.BulgeGraph.from_dotbracket(dot_bracket)\n",
    "    elements_strcode = bg.to_element_string()\n",
    "    elements_strcode += 'P'*(SEQ_MAX_LEN-len(elements_strcode))\n",
    "    map_dict = {'P':7,'s':0,'h':1,'i':2,'m':3,'f':4,'t':5}\n",
    "    elements_numcode = list(map(lambda x:map_dict[x],list(elements_strcode)))\n",
    "    return torch.tensor(elements_numcode)\n",
    "    \n",
    "class siRNA_dataset_trsfmr(Dataset):\n",
    "    def __init__(self, df_structured_encoded):\n",
    "        label_tensor = torch.tensor(list(df_structured_encoded['mRNA_remaining_pct']))\n",
    "        self.label_tensor = label_tensor.reshape([len(label_tensor),1]).to(torch.float32)\n",
    "        self.seq_sense_index = self._pad_to_equal_length(df_structured_encoded['seq_agct_int_sense'])\n",
    "        self.seq_antis_index = self._pad_to_equal_length(df_structured_encoded['seq_agct_int_anti'])\n",
    "        self.modi_sense_index = self._pad_to_equal_length(df_structured_encoded['seq_modi_int_sense'])\n",
    "        self.modi_antis_index = self._pad_to_equal_length(df_structured_encoded['seq_modi_int_anti'])\n",
    "        self.struct_sense_index = torch.stack(list(df_structured_encoded['dp_mfe_sense'].apply(get_nt_strtype_vec)))\n",
    "        self.struct_sense_index = torch.flip(self.struct_sense_index,dims=[1])\n",
    "        self.struct_antis_index = torch.stack(list(df_structured_encoded['dp_mfe_antis'].apply(get_nt_strtype_vec)))\n",
    "        df_tabular_encoded = df_structured_encoded.loc[:,df_structured_encoded.columns.str.contains(r'!\\w+!')]\n",
    "        self.features_tensor = torch.tensor(df_tabular_encoded.values).to(torch.float32)\n",
    "        self.domain_label_A = np.array(df_structured_encoded['publication_id'])\n",
    "    def __getitem__(self,index):\n",
    "        return (self.seq_sense_index[index],self.seq_antis_index[index],\n",
    "                self.modi_sense_index[index],self.modi_antis_index[index],\n",
    "                self.struct_sense_index[index],self.struct_antis_index[index],\n",
    "                self.features_tensor[index],self.label_tensor[index],self.domain_label_A[index])\n",
    "    def __len__(self):\n",
    "        return self.label_tensor.size(0)\n",
    "    def _pad_to_equal_length(self,series, pad_value=7, length=28):\n",
    "        num_list = series.str.split('').str[1:-1]\n",
    "        padded = num_list.apply(lambda x: x + [str(pad_value)] * (length - len(x)))\n",
    "        matrix = np.array(padded.tolist(), dtype=int)\n",
    "        return torch.tensor(matrix)\n",
    "\n",
    "def get_nt_strtype_vec(dot_bracket,SEQ_MAX_LEN=28):\n",
    "    import forgi.graph.bulge_graph as fgb\n",
    "    bg = fgb.BulgeGraph.from_dotbracket(dot_bracket)\n",
    "    elements_strcode = bg.to_element_string()\n",
    "    elements_strcode += 'P'*(SEQ_MAX_LEN-len(elements_strcode))\n",
    "    map_dict = {'P':7,'s':0,'h':1,'i':2,'m':3,'f':4,'t':5}\n",
    "    elements_numcode = list(map(lambda x:map_dict[x],list(elements_strcode)))\n",
    "    return torch.tensor(elements_numcode)\n",
    "    \n",
    "class siRNA_dataset_for_trsfmr(Dataset):\n",
    "    def __init__(self, df_encoded,df_struct):\n",
    "        assert False not in (df_encoded.index == df_struct.index)\n",
    "        label_tensor = torch.tensor(list(df_struct['mRNA_remaining_pct']))\n",
    "        self.label_tensor = label_tensor.reshape([len(label_tensor),1]).to(torch.float32)\n",
    "        self.seq_sense_index = self._pad_to_equal_length(df_struct['seq_agct_int_sense'])\n",
    "        self.seq_antis_index = self._pad_to_equal_length(df_struct['seq_agct_int_anti'])\n",
    "        self.modi_sense_index = self._pad_to_equal_length(df_struct['seq_modi_int_sense'])\n",
    "        self.modi_antis_index = self._pad_to_equal_length(df_struct['seq_modi_int_anti'])\n",
    "        self.struct_sense_index = torch.stack(list(df_struct['!!dp_mfe_sense'].apply(get_nt_strtype_vec)))\n",
    "        self.struct_antis_index = torch.stack(list(df_struct['!!dp_mfe_antis'].apply(get_nt_strtype_vec)))\n",
    "        self.features_tensor = torch.tensor(df_encoded.values).to(torch.float32)\n",
    "        self.domain_label_A = np.array(df_struct['publication_id'])\n",
    "    def __getitem__(self,index):\n",
    "        return (self.seq_sense_index[index],self.seq_antis_index[index],\n",
    "                self.modi_sense_index[index],self.modi_antis_index[index],\n",
    "                self.struct_sense_index[index],self.struct_antis_index[index],\n",
    "                self.features_tensor[index],self.label_tensor[index],self.domain_label_A[index])\n",
    "    def __len__(self):\n",
    "        return self.label_tensor.size(0)\n",
    "    def _pad_to_equal_length(self,series, pad_value=7, length=28):\n",
    "        num_list = series.str.split('').str[1:-1]\n",
    "        padded = num_list.apply(lambda x: x + [str(pad_value)] * (length - len(x)))\n",
    "        matrix = np.array(padded.tolist(), dtype=int)\n",
    "        return torch.tensor(matrix)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=48, max_len=28):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043e485c-a686-438f-88d4-57da9a4677bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class siRNATransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=32, d_model=32, nhead=16, num_layers=2, max_len=28, dropout=0.1, ablation=False):\n",
    "        super().__init__()\n",
    "        print('ablation:', ablation)\n",
    "        self.ablation = ablation\n",
    "        self.pad_idx = 7\n",
    "        self.seq_embed    = nn.Embedding(8, embed_dim, padding_idx=self.pad_idx)\n",
    "        self.modi_embed   = nn.Embedding(8, embed_dim, padding_idx=self.pad_idx)\n",
    "        self.struct_embed = nn.Embedding(8, embed_dim, padding_idx=self.pad_idx)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 3, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, 3)\n",
    "        )\n",
    "\n",
    "        self.combine_fc = nn.Linear(embed_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ff_dim = max(4 * d_model, 128)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 16)\n",
    "        )\n",
    "\n",
    "        assert d_model % nhead == 0, f\"d_model({d_model}) must be divisible by nhead({nhead})\"\n",
    "        nn.init.xavier_uniform_(self.combine_fc.weight)\n",
    "        nn.init.zeros_(self.combine_fc.bias)\n",
    "        for m in self.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, seq, modi, struct):\n",
    "        seq_emb    = self.seq_embed(seq)\n",
    "        modi_emb   = self.modi_embed(modi)\n",
    "        struct_emb = self.struct_embed(struct)\n",
    "        emb_cat = torch.cat([seq_emb, modi_emb, struct_emb], dim=-1)\n",
    "        gate_w = self.gate(emb_cat).sigmoid()\n",
    "        if self.ablation:\n",
    "            fused = gate_w[..., 0:1] * seq_emb + gate_w[..., 1:2] * modi_emb\n",
    "        else:\n",
    "            fused = (gate_w[..., 0:1] * seq_emb +\n",
    "                     gate_w[..., 1:2] * modi_emb +\n",
    "                     gate_w[..., 2:3] * struct_emb)\n",
    "        features = self.combine_fc(fused)\n",
    "        features = self.pos_encoder(features)\n",
    "        features = self.dropout(features)\n",
    "        pad_mask = (seq == self.pad_idx)\n",
    "        enc = self.transformer(features, src_key_padding_mask=pad_mask)\n",
    "        valid = (~pad_mask).unsqueeze(-1).float()\n",
    "        enc = enc * valid\n",
    "        denom = valid.sum(dim=1).clamp_min(1e-6)\n",
    "        pooled = enc.sum(dim=1) / denom\n",
    "        logits = self.head(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e49a5e4-f611-4998-82ab-6b8b47c670c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfxMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(109,256)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(256,128)\n",
    "        self.actv2 = nn.ReLU()\n",
    "        self.dense3 = nn.Linear(128,16)\n",
    "        self.actv3 = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):    \n",
    "        x = self.dense1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.actv3(x)\n",
    "        return x.view(x.size(0),-1)\n",
    "\n",
    "#summary(TfxMLP(), input_size=(BATCH_SIZE, 109))\n",
    "\n",
    "class CombineMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(48,128)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(128,128)\n",
    "        self.actv2 = nn.ReLU()\n",
    "        self.dense3 = nn.Linear(128,64)\n",
    "        self.actv3 = nn.ReLU()\n",
    "        self.actv_linear = nn.Linear(64,1)\n",
    "    def forward(self,x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.actv3(x)\n",
    "        x = F.dropout(x,p=0.5,training=self.training)\n",
    "        x = self.actv_linear(x)\n",
    "        return x.view(x.size(0),-1)\n",
    "\n",
    "#summary(CombineMLP(), input_size=(BATCH_SIZE, 48))\n",
    "\n",
    "class modisiR_transformer(nn.Module):\n",
    "    def __init__(self,ablation):\n",
    "        super().__init__()\n",
    "        self.transformer_sense = siRNATransformer(ablation=ablation)\n",
    "        self.transformer_antis = siRNATransformer(ablation=ablation)\n",
    "        self.tfx_mlp = TfxMLP()\n",
    "        self.combine_mlp = CombineMLP()\n",
    "    def forward(self,seq_sense,seq_antis,modi_sense,modi_antis,struct_sense,struct_antis,x_tfx):\n",
    "        modiseqstr_sense = self.transformer_sense(seq_sense,modi_sense,struct_sense)\n",
    "        modiseqstr_antis = self.transformer_antis(seq_antis,modi_antis,struct_antis)\n",
    "        tfx_embed = self.tfx_mlp(x_tfx)\n",
    "        x_combine = torch.cat([modiseqstr_sense,modiseqstr_antis,tfx_embed],axis=1)\n",
    "        y_pred = self.combine_mlp(x_combine)\n",
    "        return y_pred.reshape([len(y_pred),1])\n",
    "\n",
    "#summary(modisiR_3dConv(), input_size=((256, 2,28,6,7),(256,1,28,7),(256,109)))\n",
    "\n",
    "def train_ERM(dataload_TRAIN,model,optimizer,criterion):\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    loss_train = 0\n",
    "    \n",
    "    for seq_sense_index, seq_antis_index, modi_sense_index, modi_antis_index, struct_sense_index, struct_antis_index, features_tensor, label_tensor, domain_label_A in dataload_TRAIN:\n",
    "        seq_sense_index = seq_sense_index.to(DEVICE)\n",
    "        seq_antis_index = seq_antis_index.to(DEVICE)        \n",
    "        modi_sense_index = modi_sense_index.to(DEVICE)\n",
    "        modi_antis_index = modi_antis_index.to(DEVICE)\n",
    "        struct_sense_index = struct_sense_index.to(DEVICE)\n",
    "        struct_antis_index = struct_antis_index.to(DEVICE)\n",
    "        features_tensor = features_tensor.to(DEVICE)\n",
    "        y_batch_lbl = label_tensor.to(DEVICE)\n",
    "        \n",
    "        y_batch_pred = model(seq_sense_index, seq_antis_index, modi_sense_index, modi_antis_index, \n",
    "                             struct_sense_index, struct_antis_index, features_tensor)\n",
    "        loss_batch = criterion(y_batch_pred,y_batch_lbl)\n",
    "        optimizer.zero_grad()\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss_batch.item()\n",
    "    return loss_train/len(dataload_TRAIN)\n",
    "\n",
    "def train_VREX(dataload_TRAIN,env_list,beta,model,optimizer,criterion):\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    \n",
    "    for seq_sense_index, seq_antis_index, modi_sense_index, modi_antis_index, struct_sense_index, struct_antis_index, features_tensor, label_tensor, domain_label_A in dataload_TRAIN:\n",
    "        risks = []\n",
    "        x_domain_label_A = np.array(domain_label_A)\n",
    "        for env in env_list:\n",
    "            env_mask = (x_domain_label_A == env)\n",
    "            if True not in env_mask: continue\n",
    "            x_seq_sense_index = seq_sense_index[env_mask].to(DEVICE)\n",
    "            x_seq_antis_index = seq_antis_index[env_mask].to(DEVICE)        \n",
    "            x_modi_sense_index = modi_sense_index[env_mask].to(DEVICE)\n",
    "            x_modi_antis_index = modi_antis_index[env_mask].to(DEVICE)\n",
    "            x_struct_sense_index = struct_sense_index[env_mask].to(DEVICE)\n",
    "            x_struct_antis_index = struct_antis_index[env_mask].to(DEVICE)\n",
    "            x_features_tensor = features_tensor[env_mask].to(DEVICE)\n",
    "            y_batch_lbl = label_tensor[env_mask].to(DEVICE)\n",
    "            y_batch_pred = model(x_seq_sense_index, x_seq_antis_index, x_modi_sense_index, x_modi_antis_index, \n",
    "                                 x_struct_sense_index, x_struct_antis_index, x_features_tensor)\n",
    "            risks.append(criterion(y_batch_pred,y_batch_lbl))\n",
    "        \n",
    "        risks = torch.stack(risks)\n",
    "        risks_mean = torch.mean(risks)\n",
    "        risks_var = torch.var(risks)\n",
    "        \n",
    "        loss_batch = risks_mean + beta * risks_var\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train_total += risks_mean.item()\n",
    "    return loss_train_total/len(dataload_TRAIN)\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, threshold=30):\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    y_true = y_true.clip(0,100)\n",
    "    y_pred = y_pred.clip(0,100)\n",
    "    \n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    y_true_binary = (y_true < threshold).astype(int)\n",
    "    y_pred_binary = (y_pred < threshold).astype(int)\n",
    "    \n",
    "    mask = (y_pred >= 0) & (y_pred <= threshold)\n",
    "    range_mae = mean_absolute_error(y_true[mask], y_pred[mask]) if mask.sum() > 0 else 100\n",
    "\n",
    "    precision = precision_score(y_true_binary, y_pred_binary, average='binary')\n",
    "    recall = recall_score(y_true_binary, y_pred_binary, average='binary')\n",
    "    \n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    score = (1 - mae / 100) * 0.5 + (1 - range_mae / 100) * f1 * 0.5\n",
    "    \n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return score\n",
    "\n",
    "def validate(dataload_VAL,model,criterion,threshold=30):\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "    y_val_lbl = []\n",
    "    y_val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for seq_sense_index, seq_antis_index, modi_sense_index, modi_antis_index, struct_sense_index, struct_antis_index, features_tensor, label_tensor, domain_label_A in dataload_VAL:\n",
    "            seq_sense_index = seq_sense_index.to(DEVICE)\n",
    "            seq_antis_index = seq_antis_index.to(DEVICE)        \n",
    "            modi_sense_index = modi_sense_index.to(DEVICE)\n",
    "            modi_antis_index = modi_antis_index.to(DEVICE)\n",
    "            struct_sense_index = struct_sense_index.to(DEVICE)\n",
    "            struct_antis_index = struct_antis_index.to(DEVICE)\n",
    "            features_tensor = features_tensor.to(DEVICE)\n",
    "            y_batch_lbl = label_tensor.to(DEVICE)\n",
    "\n",
    "            y_batch_pred = model(seq_sense_index, seq_antis_index, modi_sense_index, modi_antis_index, \n",
    "                                 struct_sense_index, struct_antis_index, features_tensor)\n",
    "            loss_batch = criterion(y_batch_pred,y_batch_lbl)\n",
    "\n",
    "            loss_val += loss_batch.item()\n",
    "            y_val_lbl.extend(y_batch_lbl.cpu().numpy())\n",
    "            y_val_pred.extend(y_batch_pred.cpu().numpy())\n",
    "        \n",
    "    y_val_pred = np.array(y_val_pred)\n",
    "    y_val_lbl = np.array(y_val_lbl)\n",
    "    model_score = calculate_metrics(y_val_pred, y_val_lbl,threshold)\n",
    "    return loss_val/len(dataload_VAL),model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f836b91-fde6-45f5-854c-0e73567275dc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d842d6-4b6c-4371-a8ce-8085476d5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_TEST_IID = siRNA_dataset_trsfmr(df_structured_encoded_iid_test)\n",
    "dataset_TEST_OOD = siRNA_dataset_trsfmr(df_structured_encoded_ood_test)\n",
    "dataset_TRVL = siRNA_dataset_trsfmr(df_structured_encoded_iid_trvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8148bd6-16d2-4568-a723-155eff0d37bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def cvmodel_transformer_test(model_list,dataset_TEST,ablation):\n",
    "    model4test = modisiR_transformer(ablation)\n",
    "    test_score_list = []\n",
    "    for i in range(len(model_list)):\n",
    "        model4test.load_state_dict(model_list[i])\n",
    "        model4test.eval()\n",
    "        y_pred_TEST = model4test(dataset_TEST.seq_sense_index, dataset_TEST.seq_antis_index, \n",
    "                                 dataset_TEST.modi_sense_index, dataset_TEST.modi_antis_index, \n",
    "                                 dataset_TEST.struct_sense_index, dataset_TEST.struct_antis_index, \n",
    "                                 dataset_TEST.features_tensor)\n",
    "        test_score = calculate_metrics(y_pred_TEST.detach().numpy(),dataset_TEST.label_tensor.detach().numpy())\n",
    "        print(test_score)\n",
    "        test_score_list.append(test_score)\n",
    "    return test_score_list\n",
    "\n",
    "def cv_train(model_type:str,dataset_TRVL,dataset_TEST_IID,dataset_TEST_OOD,ablation,\n",
    "             model_list,cv_log):\n",
    "    lr = 0.002\n",
    "    EPOCHS = 150\n",
    "    BETA = 1\n",
    "    print('beta:',BETA)\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    early_stop_score = 1\n",
    "    warm_up_epoch_num = 10\n",
    "    loss_tolerance_epoch_num = 10\n",
    "    env_list = df_structured_encoded_iid_trvl['publication_id'].unique() #####\n",
    "\n",
    "    dataload_TEST_IID = DataLoader(dataset=dataset_TEST_IID,batch_size=BATCH_SIZE)\n",
    "    dataload_TEST_OOD = DataLoader(dataset=dataset_TEST_OOD,batch_size=BATCH_SIZE)\n",
    "\n",
    "    kfold = KFold(n_splits=5,shuffle=True)\n",
    "    splits = kfold.split(dataset_TRVL)\n",
    "\n",
    "    for train_index, val_index in splits:\n",
    "\n",
    "        log_train = []\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        dataset_TRAIN = Subset(dataset_TRVL,train_index)\n",
    "        dataset_VAL = Subset(dataset_TRVL,val_index)\n",
    "\n",
    "        dataload_TRAIN = DataLoader(dataset=dataset_TRAIN,batch_size=BATCH_SIZE)\n",
    "        dataload_VAL = DataLoader(dataset=dataset_VAL,batch_size=BATCH_SIZE)\n",
    "\n",
    "        lowest_loss_epoch = {'loss_val':float(\"inf\"),'epoch':0}\n",
    "        best_score = -float('inf')\n",
    "        best_OOD = -float('inf')\n",
    "        model = modisiR_transformer(ablation)\n",
    "        optimizer = optim.AdamW([{'params':model.parameters(),'lr':lr}])\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            start_time_epoch = time.time()\n",
    "            if model_type == 'vrex':\n",
    "                loss_train = train_VREX(dataload_TRAIN,env_list,BETA,model,optimizer,criterion)\n",
    "            elif model_type == 'erm':\n",
    "                loss_train = train_ERM(dataload_TRAIN,model,optimizer,criterion)\n",
    "            elif model_type == 'stdrex':\n",
    "                loss_train = train_stdREX(dataload_TRAIN,env_list,BETA,model,optimizer,criterion)\n",
    "            else: print('No such model type. Type should be erm or vrex.')\n",
    "            loss_val,model_score = validate(dataload_VAL,model,criterion)\n",
    "            \n",
    "            _,test_score_iid = validate(dataload_TEST_IID,model,criterion)\n",
    "            _,test_score_ood = validate(dataload_TEST_OOD,model,criterion)\n",
    "\n",
    "            if epoch > warm_up_epoch_num:\n",
    "                if loss_val < lowest_loss_epoch['loss_val']:\n",
    "                    lowest_loss_epoch['epoch'] = epoch\n",
    "                    lowest_loss_epoch['loss_val'] = loss_val\n",
    "                elif (epoch-lowest_loss_epoch['epoch']) >= loss_tolerance_epoch_num:\n",
    "                    lowest_loss_epoch['epoch'] = epoch\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] *= 0.5\n",
    "                        \n",
    "            log_train.append((epoch,loss_train,loss_val,model_score,test_score_iid,test_score_ood,lr))\n",
    "            \n",
    "            if model_score > best_score:\n",
    "                best_score = model_score\n",
    "                best_OOD = test_score_ood\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            print(f'{epoch}\\t{model_score:.4f}\\t{test_score_iid:.4f}\\t{test_score_ood:.4f}\\t({best_score:.4f},{best_OOD:.4f})',sep='',end='\\n')\n",
    "            \n",
    "            if best_score >= early_stop_score:\n",
    "                break\n",
    "        model_list.append(best_model)\n",
    "        cv_log.append(log_train)\n",
    "        print('')\n",
    "        \n",
    "    df_cv_log = pd.DataFrame()\n",
    "    for i in range(len(cv_log)):\n",
    "        df_log = pd.DataFrame(cv_log[i])\n",
    "        mindex = pd.MultiIndex.from_product([['Model_'+str(i)],['epoch','loss_train','loss_val','val_score','iid_score','ood_score','lr']])\n",
    "        df_log.columns = mindex\n",
    "        df_cv_log = pd.concat([df_cv_log,df_log],axis=1)\n",
    "        \n",
    "    return model_list,df_cv_log\n",
    "\n",
    "def save_state_dict_2cpu(PATH_SAVE,model_list,model):\n",
    "    os.mkdir(PATH_SAVE+'models')\n",
    "    for i in range(len(model_list)):\n",
    "        print(i,list(model_list[i].values())[0].device,end='\\t')\n",
    "        model.load_state_dict(model_list[i])\n",
    "        model.to('cpu')\n",
    "        print('to',list(model.state_dict().values())[0].device)\n",
    "        torch.save(model.state_dict(), PATH_SAVE+'models/state_dict_cpu_'+str(i)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbb6d9-e109-45dc-8b2c-6f4cb561c91f",
   "metadata": {},
   "source": [
    "### train_ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f8cdd9-e7ce-497f-82a5-cd609a63e348",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 1\n",
      "ablation: False\n",
      "ablation: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tnan\tnan\tnan\t(-inf,-inf)\n",
      "1\t0.5366\t0.5209\t0.6715\t(0.5366,0.6715)\n",
      "2\t0.6066\t0.6139\t0.6528\t(0.6066,0.6528)\n",
      "3\t0.6370\t0.6464\t0.6340\t(0.6370,0.6340)\n",
      "4\t0.6302\t0.6324\t0.6234\t(0.6370,0.6340)\n",
      "5\t0.6718\t0.6772\t0.6210\t(0.6718,0.6210)\n",
      "6\t0.6970\t0.7020\t0.5943\t(0.6970,0.5943)\n",
      "7\t0.6946\t0.7048\t0.5941\t(0.6970,0.5943)\n",
      "8\t0.7165\t0.7294\t0.5725\t(0.7165,0.5725)\n",
      "9\t0.7018\t0.7119\t0.5809\t(0.7165,0.5725)\n",
      "10\t0.7119\t0.7203\t0.5733\t(0.7165,0.5725)\n",
      "11\t0.7330\t0.7429\t0.5717\t(0.7330,0.5717)\n",
      "12\t0.7178\t0.7283\t0.5668\t(0.7330,0.5717)\n",
      "13\t0.7211\t0.7289\t0.5621\t(0.7330,0.5717)\n",
      "14\t0.7016\t0.7042\t0.5935\t(0.7330,0.5717)\n",
      "15\t0.7202\t0.7330\t0.5722\t(0.7330,0.5717)\n",
      "16\t0.7126\t0.7285\t0.5771\t(0.7330,0.5717)\n",
      "17\t0.7351\t0.7476\t0.5569\t(0.7351,0.5569)\n",
      "18\t0.7203\t0.7274\t0.5813\t(0.7351,0.5569)\n",
      "19\t0.7441\t0.7506\t0.5490\t(0.7441,0.5490)\n",
      "20\t0.7389\t0.7476\t0.5636\t(0.7441,0.5490)\n",
      "21\t0.7303\t0.7402\t0.5707\t(0.7441,0.5490)\n",
      "22\t0.7342\t0.7421\t0.5660\t(0.7441,0.5490)\n",
      "23\t0.7262\t0.7361\t0.5818\t(0.7441,0.5490)\n",
      "24\t0.7341\t0.7386\t0.5835\t(0.7441,0.5490)\n",
      "25\t0.7259\t0.7431\t0.5758\t(0.7441,0.5490)\n",
      "26\t0.7534\t0.7596\t0.5697\t(0.7534,0.5697)\n",
      "27\t0.7245\t0.7349\t0.5819\t(0.7534,0.5697)\n",
      "28\t0.7483\t0.7495\t0.5616\t(0.7534,0.5697)\n",
      "29\t0.7447\t0.7550\t0.5632\t(0.7534,0.5697)\n",
      "30\t0.7567\t0.7602\t0.5646\t(0.7567,0.5646)\n",
      "31\t0.7520\t0.7603\t0.5751\t(0.7567,0.5646)\n",
      "32\t0.7553\t0.7610\t0.5670\t(0.7567,0.5646)\n",
      "33\t0.7557\t0.7588\t0.5794\t(0.7567,0.5646)\n",
      "34\t0.7526\t0.7602\t0.5734\t(0.7567,0.5646)\n",
      "35\t0.7522\t0.7654\t0.5826\t(0.7567,0.5646)\n",
      "36\t0.7599\t0.7675\t0.5882\t(0.7599,0.5882)\n",
      "37\t0.7557\t0.7609\t0.5708\t(0.7599,0.5882)\n",
      "38\t0.7611\t0.7668\t0.5667\t(0.7611,0.5667)\n",
      "39\t0.7501\t0.7574\t0.5801\t(0.7611,0.5667)\n",
      "40\t0.7603\t0.7627\t0.5773\t(0.7611,0.5667)\n",
      "41\t0.7604\t0.7610\t0.5818\t(0.7611,0.5667)\n",
      "42\t0.7703\t0.7682\t0.5766\t(0.7703,0.5766)\n",
      "43\t0.7734\t0.7722\t0.5741\t(0.7734,0.5741)\n",
      "44\t0.7600\t0.7701\t0.5838\t(0.7734,0.5741)\n",
      "45\t0.7587\t0.7639\t0.6021\t(0.7734,0.5741)\n",
      "46\t0.7579\t0.7654\t0.5858\t(0.7734,0.5741)\n",
      "47\t0.7598\t0.7660\t0.5974\t(0.7734,0.5741)\n",
      "48\t0.7713\t0.7748\t0.5899\t(0.7734,0.5741)\n",
      "49\t0.7562\t0.7614\t0.5979\t(0.7734,0.5741)\n",
      "50\t0.7720\t0.7756\t0.6044\t(0.7734,0.5741)\n",
      "51\t0.7638\t0.7664\t0.6017\t(0.7734,0.5741)\n",
      "52\t0.7561\t0.7568\t0.6240\t(0.7734,0.5741)\n",
      "53\t0.7697\t0.7702\t0.6028\t(0.7734,0.5741)\n",
      "54\t0.7707\t0.7689\t0.6089\t(0.7734,0.5741)\n",
      "55\t0.7790\t0.7744\t0.5940\t(0.7790,0.5940)\n",
      "56\t0.7776\t0.7774\t0.6072\t(0.7790,0.5940)\n",
      "57\t0.7754\t0.7750\t0.6034\t(0.7790,0.5940)\n",
      "58\t0.7765\t0.7772\t0.6088\t(0.7790,0.5940)\n",
      "59\t0.7825\t0.7849\t0.5858\t(0.7825,0.5858)\n",
      "60\t0.7763\t0.7791\t0.6046\t(0.7825,0.5858)\n",
      "61\t0.7787\t0.7857\t0.5943\t(0.7825,0.5858)\n",
      "62\t0.7751\t0.7842\t0.5923\t(0.7825,0.5858)\n",
      "63\t0.7763\t0.7812\t0.6033\t(0.7825,0.5858)\n",
      "64\t0.7764\t0.7785\t0.6151\t(0.7825,0.5858)\n",
      "65\t0.7800\t0.7823\t0.6019\t(0.7825,0.5858)\n",
      "66\t0.7860\t0.7879\t0.6064\t(0.7860,0.6064)\n",
      "67\t0.7781\t0.7858\t0.6019\t(0.7860,0.6064)\n",
      "68\t0.7833\t0.7888\t0.6066\t(0.7860,0.6064)\n",
      "69\t0.7811\t0.7917\t0.6076\t(0.7860,0.6064)\n",
      "70\t0.7732\t0.7811\t0.6214\t(0.7860,0.6064)\n",
      "71\t0.7762\t0.7790\t0.6170\t(0.7860,0.6064)\n",
      "72\t0.7741\t0.7779\t0.6177\t(0.7860,0.6064)\n",
      "73\t0.7839\t0.7816\t0.6166\t(0.7860,0.6064)\n",
      "74\t0.7857\t0.7928\t0.5981\t(0.7860,0.6064)\n",
      "75\t0.7857\t0.7894\t0.5948\t(0.7860,0.6064)\n",
      "76\t0.7902\t0.7928\t0.5948\t(0.7902,0.5948)\n",
      "77\t0.7872\t0.7905\t0.5929\t(0.7902,0.5948)\n",
      "78\t0.7860\t0.7992\t0.5994\t(0.7902,0.5948)\n",
      "79\t0.7871\t0.8025\t0.5951\t(0.7902,0.5948)\n",
      "80\t0.7889\t0.7995\t0.5950\t(0.7902,0.5948)\n",
      "81\t0.7845\t0.7978\t0.5930\t(0.7902,0.5948)\n",
      "82\t0.7890\t0.7981\t0.5951\t(0.7902,0.5948)\n",
      "83\t0.7910\t0.8000\t0.5989\t(0.7910,0.5989)\n",
      "84\t0.7896\t0.8013\t0.5997\t(0.7910,0.5989)\n",
      "85\t0.7917\t0.7998\t0.5918\t(0.7917,0.5918)\n",
      "86\t0.7929\t0.7975\t0.6011\t(0.7929,0.6011)\n",
      "87\t0.7895\t0.7998\t0.5968\t(0.7929,0.6011)\n",
      "88\t0.7882\t0.8016\t0.6061\t(0.7929,0.6011)\n",
      "89\t0.7922\t0.8008\t0.6013\t(0.7929,0.6011)\n",
      "90\t0.7948\t0.8018\t0.6038\t(0.7948,0.6038)\n",
      "91\t0.7938\t0.7999\t0.6029\t(0.7948,0.6038)\n",
      "92\t0.7898\t0.8049\t0.6044\t(0.7948,0.6038)\n",
      "93\t0.7929\t0.8040\t0.6057\t(0.7948,0.6038)\n",
      "94\t0.7922\t0.8001\t0.6087\t(0.7948,0.6038)\n",
      "95\t0.7961\t0.8054\t0.6065\t(0.7961,0.6065)\n",
      "96\t0.7909\t0.8025\t0.6029\t(0.7961,0.6065)\n",
      "97\t0.7913\t0.7963\t0.6007\t(0.7961,0.6065)\n",
      "98\t0.7973\t0.8044\t0.6077\t(0.7973,0.6077)\n",
      "99\t0.7917\t0.8036\t0.6058\t(0.7973,0.6077)\n",
      "100\t0.7970\t0.8084\t0.6012\t(0.7973,0.6077)\n",
      "101\t0.7970\t0.8065\t0.6123\t(0.7973,0.6077)\n",
      "102\t0.7962\t0.8067\t0.6054\t(0.7973,0.6077)\n",
      "103\t0.8035\t0.8073\t0.6063\t(0.8035,0.6063)\n",
      "104\t0.8025\t0.8077\t0.6041\t(0.8035,0.6063)\n",
      "105\t0.8025\t0.8076\t0.6057\t(0.8035,0.6063)\n",
      "106\t0.8011\t0.8051\t0.6029\t(0.8035,0.6063)\n",
      "107\t0.8015\t0.8075\t0.6041\t(0.8035,0.6063)\n",
      "108\t0.8027\t0.8123\t0.6027\t(0.8035,0.6063)\n",
      "109\t0.8024\t0.8119\t0.6045\t(0.8035,0.6063)\n",
      "110\t0.8059\t0.8113\t0.6058\t(0.8059,0.6058)\n",
      "111\t0.8046\t0.8117\t0.6051\t(0.8059,0.6058)\n",
      "112\t0.8024\t0.8128\t0.6092\t(0.8059,0.6058)\n",
      "113\t0.8039\t0.8071\t0.6095\t(0.8059,0.6058)\n",
      "114\t0.8041\t0.8075\t0.6090\t(0.8059,0.6058)\n",
      "115\t0.8067\t0.8111\t0.6120\t(0.8067,0.6120)\n",
      "116\t0.8066\t0.8138\t0.6041\t(0.8067,0.6120)\n",
      "117\t0.8010\t0.8109\t0.6119\t(0.8067,0.6120)\n",
      "118\t0.8047\t0.8103\t0.6133\t(0.8067,0.6120)\n",
      "119\t0.8018\t0.8141\t0.6067\t(0.8067,0.6120)\n",
      "120\t0.8022\t0.8103\t0.6128\t(0.8067,0.6120)\n",
      "121\t0.8020\t0.8153\t0.6027\t(0.8067,0.6120)\n",
      "122\t0.8067\t0.8095\t0.6059\t(0.8067,0.6120)\n",
      "123\t0.8079\t0.8112\t0.6102\t(0.8079,0.6102)\n",
      "124\t0.8056\t0.8124\t0.6062\t(0.8079,0.6102)\n",
      "125\t0.8034\t0.8100\t0.6108\t(0.8079,0.6102)\n",
      "126\t0.8048\t0.8130\t0.6055\t(0.8079,0.6102)\n",
      "127\t0.8067\t0.8114\t0.6046\t(0.8079,0.6102)\n",
      "128\t0.8055\t0.8104\t0.6084\t(0.8079,0.6102)\n",
      "129\t0.8063\t0.8062\t0.6099\t(0.8079,0.6102)\n",
      "130\t0.8060\t0.8059\t0.6122\t(0.8079,0.6102)\n",
      "131\t0.8081\t0.8133\t0.6134\t(0.8081,0.6134)\n",
      "132\t0.8071\t0.8134\t0.6091\t(0.8081,0.6134)\n",
      "133\t0.8068\t0.8125\t0.6124\t(0.8081,0.6134)\n",
      "134\t0.8070\t0.8119\t0.6131\t(0.8081,0.6134)\n",
      "135\t0.8073\t0.8119\t0.6133\t(0.8081,0.6134)\n",
      "136\t0.8050\t0.8129\t0.6118\t(0.8081,0.6134)\n",
      "137\t0.8077\t0.8110\t0.6113\t(0.8081,0.6134)\n",
      "138\t0.8078\t0.8126\t0.6129\t(0.8081,0.6134)\n",
      "139\t0.8079\t0.8144\t0.6062\t(0.8081,0.6134)\n",
      "140\t0.8060\t0.8096\t0.6115\t(0.8081,0.6134)\n",
      "141\t0.8082\t0.8082\t0.6129\t(0.8082,0.6129)\n",
      "142\t0.8069\t0.8107\t0.6156\t(0.8082,0.6129)\n",
      "143\t0.8056\t0.8093\t0.6119\t(0.8082,0.6129)\n",
      "144\t0.8061\t0.8083\t0.6143\t(0.8082,0.6129)\n",
      "145\t0.8033\t0.8118\t0.6162\t(0.8082,0.6129)\n",
      "146\t0.8063\t0.8156\t0.6081\t(0.8082,0.6129)\n",
      "147\t0.8106\t0.8125\t0.6193\t(0.8106,0.6193)\n",
      "148\t0.8074\t0.8151\t0.6158\t(0.8106,0.6193)\n",
      "149\t0.8060\t0.8132\t0.6181\t(0.8106,0.6193)\n",
      "\n",
      "ablation: False\n",
      "ablation: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tnan\tnan\tnan\t(-inf,-inf)\n",
      "1\t0.5280\t0.5284\t0.6535\t(0.5280,0.6535)\n",
      "2\t0.5694\t0.5710\t0.6263\t(0.5694,0.6263)\n",
      "3\t0.6025\t0.6091\t0.6177\t(0.6025,0.6177)\n",
      "4\t0.5707\t0.5688\t0.5808\t(0.6025,0.6177)\n",
      "5\t0.6330\t0.6412\t0.6025\t(0.6330,0.6025)\n",
      "6\t0.6902\t0.6953\t0.5656\t(0.6902,0.5656)\n",
      "7\t0.6944\t0.7002\t0.5671\t(0.6944,0.5671)\n",
      "8\t0.6991\t0.7024\t0.5578\t(0.6991,0.5578)\n",
      "9\t0.7172\t0.7171\t0.5206\t(0.7172,0.5206)\n",
      "10\t0.7165\t0.7194\t0.5195\t(0.7172,0.5206)\n",
      "11\t0.7294\t0.7308\t0.5043\t(0.7294,0.5043)\n",
      "12\t0.7106\t0.7187\t0.5229\t(0.7294,0.5043)\n",
      "13\t0.7092\t0.7162\t0.5080\t(0.7294,0.5043)\n",
      "14\t0.7396\t0.7487\t0.5124\t(0.7396,0.5124)\n",
      "15\t0.7141\t0.7204\t0.5113\t(0.7396,0.5124)\n",
      "16\t0.7390\t0.7437\t0.5049\t(0.7396,0.5124)\n",
      "17\t0.7336\t0.7425\t0.5132\t(0.7396,0.5124)\n",
      "18\t0.7261\t0.7380\t0.5110\t(0.7396,0.5124)\n",
      "19\t0.7211\t0.7342\t0.5204\t(0.7396,0.5124)\n",
      "20\t0.7392\t0.7415\t0.5155\t(0.7396,0.5124)\n",
      "21\t0.7208\t0.7322\t0.5245\t(0.7396,0.5124)\n",
      "22\t0.7349\t0.7400\t0.5155\t(0.7396,0.5124)\n",
      "23\t0.7470\t0.7547\t0.5159\t(0.7470,0.5159)\n",
      "24\t0.7249\t0.7322\t0.5228\t(0.7470,0.5159)\n",
      "25\t0.7481\t0.7524\t0.5179\t(0.7481,0.5179)\n",
      "26\t0.7545\t0.7604\t0.5162\t(0.7545,0.5162)\n",
      "27\t0.7284\t0.7365\t0.5185\t(0.7545,0.5162)\n",
      "28\t0.7364\t0.7436\t0.5219\t(0.7545,0.5162)\n",
      "29\t0.7469\t0.7512\t0.5434\t(0.7545,0.5162)\n",
      "30\t0.7565\t0.7592\t0.5317\t(0.7565,0.5317)\n",
      "31\t0.7602\t0.7692\t0.5334\t(0.7602,0.5334)\n",
      "32\t0.7615\t0.7661\t0.5492\t(0.7615,0.5492)\n",
      "33\t0.7676\t0.7716\t0.5536\t(0.7676,0.5536)\n",
      "34\t0.7595\t0.7713\t0.5618\t(0.7676,0.5536)\n",
      "35\t0.7636\t0.7706\t0.5572\t(0.7676,0.5536)\n",
      "36\t0.7663\t0.7708\t0.5627\t(0.7676,0.5536)\n",
      "37\t0.7639\t0.7693\t0.5617\t(0.7676,0.5536)\n",
      "38\t0.7620\t0.7631\t0.5685\t(0.7676,0.5536)\n",
      "39\t0.7704\t0.7778\t0.5871\t(0.7704,0.5871)\n",
      "40\t0.7743\t0.7729\t0.5700\t(0.7743,0.5700)\n",
      "41\t0.7733\t0.7794\t0.5897\t(0.7743,0.5700)\n",
      "42\t0.7540\t0.7599\t0.5849\t(0.7743,0.5700)\n",
      "43\t0.7693\t0.7688\t0.6013\t(0.7743,0.5700)\n",
      "44\t0.7587\t0.7622\t0.5688\t(0.7743,0.5700)\n",
      "45\t0.7838\t0.7923\t0.5875\t(0.7838,0.5875)\n",
      "46\t0.7742\t0.7824\t0.5826\t(0.7838,0.5875)\n",
      "47\t0.7697\t0.7732\t0.5808\t(0.7838,0.5875)\n",
      "48\t0.7750\t0.7862\t0.5926\t(0.7838,0.5875)\n",
      "49\t0.7707\t0.7778\t0.5967\t(0.7838,0.5875)\n",
      "50\t0.7695\t0.7790\t0.5802\t(0.7838,0.5875)\n",
      "51\t0.7827\t0.7857\t0.5878\t(0.7838,0.5875)\n",
      "52\t0.7778\t0.7854\t0.5758\t(0.7838,0.5875)\n",
      "53\t0.7784\t0.7871\t0.5952\t(0.7838,0.5875)\n",
      "54\t0.7646\t0.7693\t0.5889\t(0.7838,0.5875)\n",
      "55\t0.7771\t0.7854\t0.5933\t(0.7838,0.5875)\n",
      "56\t0.7747\t0.7818\t0.6003\t(0.7838,0.5875)\n",
      "57\t0.7635\t0.7743\t0.6037\t(0.7838,0.5875)\n",
      "58\t0.7806\t0.7862\t0.5939\t(0.7838,0.5875)\n",
      "59\t0.7766\t0.7820\t0.5893\t(0.7838,0.5875)\n",
      "60\t0.7794\t0.7825\t0.5858\t(0.7838,0.5875)\n",
      "61\t0.7775\t0.7906\t0.6013\t(0.7838,0.5875)\n",
      "62\t0.7753\t0.7863\t0.6048\t(0.7838,0.5875)\n",
      "63\t0.7712\t0.7828\t0.5968\t(0.7838,0.5875)\n",
      "64\t0.7756\t0.7871\t0.6000\t(0.7838,0.5875)\n",
      "65\t0.7827\t0.7917\t0.5943\t(0.7838,0.5875)\n",
      "66\t0.7752\t0.7831\t0.5999\t(0.7838,0.5875)\n",
      "67\t0.7837\t0.7917\t0.6080\t(0.7838,0.5875)\n",
      "68\t0.7838\t0.7890\t0.6002\t(0.7838,0.6002)\n",
      "69\t0.7847\t0.7957\t0.6043\t(0.7847,0.6043)\n",
      "70\t0.7862\t0.7943\t0.5930\t(0.7862,0.5930)\n",
      "71\t0.7882\t0.7977\t0.5964\t(0.7882,0.5964)\n",
      "72\t0.7877\t0.7947\t0.5918\t(0.7882,0.5964)\n",
      "73\t0.7872\t0.7955\t0.5884\t(0.7882,0.5964)\n",
      "74\t0.7874\t0.7964\t0.5950\t(0.7882,0.5964)\n",
      "75\t0.7883\t0.7974\t0.5904\t(0.7883,0.5904)\n",
      "76\t0.7946\t0.7997\t0.6013\t(0.7946,0.6013)\n",
      "77\t0.7882\t0.7984\t0.5942\t(0.7946,0.6013)\n",
      "78\t0.7854\t0.7964\t0.5850\t(0.7946,0.6013)\n",
      "79\t0.7919\t0.7962\t0.5970\t(0.7946,0.6013)\n",
      "80\t0.7886\t0.7945\t0.5893\t(0.7946,0.6013)\n",
      "81\t0.7895\t0.7965\t0.5951\t(0.7946,0.6013)\n",
      "82\t0.7855\t0.7924\t0.5803\t(0.7946,0.6013)\n",
      "83\t0.7888\t0.7965\t0.5861\t(0.7946,0.6013)\n",
      "84\t0.7922\t0.8009\t0.5897\t(0.7946,0.6013)\n",
      "85\t0.7957\t0.7984\t0.5923\t(0.7957,0.5923)\n",
      "86\t0.7850\t0.7860\t0.5852\t(0.7957,0.5923)\n",
      "87\t0.7941\t0.7999\t0.5913\t(0.7957,0.5923)\n",
      "88\t0.7998\t0.8039\t0.5933\t(0.7998,0.5933)\n",
      "89\t0.7884\t0.7957\t0.5867\t(0.7998,0.5933)\n",
      "90\t0.7916\t0.7989\t0.5908\t(0.7998,0.5933)\n",
      "91\t0.7902\t0.7945\t0.5896\t(0.7998,0.5933)\n",
      "92\t0.7939\t0.7992\t0.5897\t(0.7998,0.5933)\n",
      "93\t0.7907\t0.7952\t0.5953\t(0.7998,0.5933)\n",
      "94\t0.7942\t0.8001\t0.5904\t(0.7998,0.5933)\n",
      "95\t0.7929\t0.7998\t0.5895\t(0.7998,0.5933)\n",
      "96\t0.7931\t0.7971\t0.5874\t(0.7998,0.5933)\n",
      "97\t0.7922\t0.7973\t0.6016\t(0.7998,0.5933)\n",
      "98\t0.7864\t0.7952\t0.5858\t(0.7998,0.5933)\n",
      "99\t0.8003\t0.8016\t0.6041\t(0.8003,0.6041)\n",
      "100\t0.8019\t0.8037\t0.6048\t(0.8019,0.6048)\n",
      "101\t0.8025\t0.8048\t0.6009\t(0.8025,0.6009)\n",
      "102\t0.8014\t0.8042\t0.6030\t(0.8025,0.6009)\n",
      "103\t0.8023\t0.8071\t0.6018\t(0.8025,0.6009)\n",
      "104\t0.8006\t0.8068\t0.5986\t(0.8025,0.6009)\n",
      "105\t0.8059\t0.8054\t0.6000\t(0.8059,0.6000)\n",
      "106\t0.8039\t0.8040\t0.6007\t(0.8059,0.6000)\n",
      "107\t0.8054\t0.8030\t0.6086\t(0.8059,0.6000)\n",
      "108\t0.8063\t0.8075\t0.5941\t(0.8063,0.5941)\n",
      "109\t0.8062\t0.8103\t0.5970\t(0.8063,0.5941)\n",
      "110\t0.8085\t0.8097\t0.6057\t(0.8085,0.6057)\n",
      "111\t0.8019\t0.8032\t0.5876\t(0.8085,0.6057)\n",
      "112\t0.8061\t0.8043\t0.6025\t(0.8085,0.6057)\n",
      "113\t0.8017\t0.8050\t0.5989\t(0.8085,0.6057)\n",
      "114\t0.8063\t0.8091\t0.6047\t(0.8085,0.6057)\n",
      "115\t0.8040\t0.8032\t0.5972\t(0.8085,0.6057)\n",
      "116\t0.8035\t0.8090\t0.5969\t(0.8085,0.6057)\n",
      "117\t0.8039\t0.8083\t0.6016\t(0.8085,0.6057)\n",
      "118\t0.8043\t0.8096\t0.6056\t(0.8085,0.6057)\n",
      "119\t0.8049\t0.8077\t0.6061\t(0.8085,0.6057)\n",
      "120\t0.8055\t0.8107\t0.5998\t(0.8085,0.6057)\n",
      "121\t0.8051\t0.8088\t0.6058\t(0.8085,0.6057)\n",
      "122\t0.8041\t0.8066\t0.6002\t(0.8085,0.6057)\n",
      "123\t0.8025\t0.8056\t0.6044\t(0.8085,0.6057)\n",
      "124\t0.8066\t0.8098\t0.6018\t(0.8085,0.6057)\n",
      "125\t0.8050\t0.8096\t0.6020\t(0.8085,0.6057)\n",
      "126\t0.8067\t0.8075\t0.6009\t(0.8085,0.6057)\n",
      "127\t0.8062\t0.8085\t0.6026\t(0.8085,0.6057)\n",
      "128\t0.8064\t0.8074\t0.5992\t(0.8085,0.6057)\n",
      "129\t0.8049\t0.8073\t0.6069\t(0.8085,0.6057)\n",
      "130\t0.8067\t0.8067\t0.6086\t(0.8085,0.6057)\n",
      "131\t0.8065\t0.8077\t0.6072\t(0.8085,0.6057)\n",
      "132\t0.8042\t0.8067\t0.6052\t(0.8085,0.6057)\n",
      "133\t0.8052\t0.8077\t0.6060\t(0.8085,0.6057)\n",
      "134\t0.8044\t0.8103\t0.5980\t(0.8085,0.6057)\n",
      "135\t0.8061\t0.8063\t0.5987\t(0.8085,0.6057)\n",
      "136\t0.8059\t0.8074\t0.5985\t(0.8085,0.6057)\n",
      "137\t0.8079\t0.8095\t0.5993\t(0.8085,0.6057)\n",
      "138\t0.8051\t0.8078\t0.6044\t(0.8085,0.6057)\n",
      "139\t0.8082\t0.8115\t0.5996\t(0.8085,0.6057)\n",
      "140\t0.8083\t0.8117\t0.6042\t(0.8085,0.6057)\n",
      "141\t0.8069\t0.8120\t0.5964\t(0.8085,0.6057)\n",
      "142\t0.8055\t0.8110\t0.6021\t(0.8085,0.6057)\n",
      "143\t0.8061\t0.8099\t0.6029\t(0.8085,0.6057)\n",
      "144\t0.8063\t0.8131\t0.6032\t(0.8085,0.6057)\n",
      "145\t0.8077\t0.8097\t0.6052\t(0.8085,0.6057)\n",
      "146\t0.8044\t0.8119\t0.6010\t(0.8085,0.6057)\n",
      "147\t0.8058\t0.8123\t0.6005\t(0.8085,0.6057)\n",
      "148\t0.8081\t0.8133\t0.6030\t(0.8085,0.6057)\n",
      "149\t0.8084\t0.8118\t0.5997\t(0.8085,0.6057)\n",
      "\n",
      "ablation: False\n",
      "ablation: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tnan\tnan\tnan\t(-inf,-inf)\n",
      "1\t0.5259\t0.5128\t0.6739\t(0.5259,0.6739)\n",
      "2\t0.5334\t0.5222\t0.6271\t(0.5334,0.6271)\n",
      "3\t0.6047\t0.6072\t0.6113\t(0.6047,0.6113)\n",
      "4\t0.6619\t0.6612\t0.6092\t(0.6619,0.6092)\n",
      "5\t0.6391\t0.6433\t0.5949\t(0.6619,0.6092)\n",
      "6\t0.6799\t0.6883\t0.5625\t(0.6799,0.5625)\n",
      "7\t0.6991\t0.7012\t0.5437\t(0.6991,0.5437)\n",
      "8\t0.6931\t0.6975\t0.5405\t(0.6991,0.5437)\n",
      "9\t0.7041\t0.7109\t0.5345\t(0.7041,0.5345)\n",
      "10\t0.7053\t0.7092\t0.5341\t(0.7053,0.5341)\n",
      "11\t0.7394\t0.7383\t0.5268\t(0.7394,0.5268)\n",
      "12\t0.7260\t0.7329\t0.5303\t(0.7394,0.5268)\n",
      "13\t0.7270\t0.7321\t0.5230\t(0.7394,0.5268)\n",
      "14\t0.7356\t0.7397\t0.5261\t(0.7394,0.5268)\n",
      "15\t0.7531\t0.7556\t0.5283\t(0.7531,0.5283)\n",
      "16\t0.7365\t0.7393\t0.5334\t(0.7531,0.5283)\n",
      "17\t0.7474\t0.7482\t0.5232\t(0.7531,0.5283)\n",
      "18\t0.7541\t0.7582\t0.5348\t(0.7541,0.5348)\n",
      "19\t0.7497\t0.7482\t0.5114\t(0.7541,0.5348)\n",
      "20\t0.7547\t0.7542\t0.5219\t(0.7547,0.5219)\n",
      "21\t0.7539\t0.7562\t0.5283\t(0.7547,0.5219)\n",
      "22\t0.7629\t0.7623\t0.5412\t(0.7629,0.5412)\n",
      "23\t0.7439\t0.7411\t0.5370\t(0.7629,0.5412)\n",
      "24\t0.7594\t0.7589\t0.5271\t(0.7629,0.5412)\n",
      "25\t0.7504\t0.7524\t0.5308\t(0.7629,0.5412)\n",
      "26\t0.7693\t0.7723\t0.5250\t(0.7693,0.5250)\n",
      "27\t0.7605\t0.7583\t0.5423\t(0.7693,0.5250)\n",
      "28\t0.7730\t0.7700\t0.5156\t(0.7730,0.5156)\n",
      "29\t0.7713\t0.7681\t0.5417\t(0.7730,0.5156)\n",
      "30\t0.7620\t0.7672\t0.5127\t(0.7730,0.5156)\n",
      "31\t0.7735\t0.7683\t0.5368\t(0.7735,0.5368)\n",
      "32\t0.7677\t0.7671\t0.5428\t(0.7735,0.5368)\n",
      "33\t0.7726\t0.7708\t0.5286\t(0.7735,0.5368)\n",
      "34\t0.7670\t0.7720\t0.5382\t(0.7735,0.5368)\n",
      "35\t0.7539\t0.7537\t0.5393\t(0.7735,0.5368)\n",
      "36\t0.7462\t0.7435\t0.5530\t(0.7735,0.5368)\n",
      "37\t0.7567\t0.7525\t0.5477\t(0.7735,0.5368)\n",
      "38\t0.7483\t0.7422\t0.5477\t(0.7735,0.5368)\n",
      "39\t0.7423\t0.7432\t0.5527\t(0.7735,0.5368)\n",
      "40\t0.7692\t0.7733\t0.5576\t(0.7735,0.5368)\n",
      "41\t0.7543\t0.7603\t0.5572\t(0.7735,0.5368)\n",
      "42\t0.7653\t0.7692\t0.5451\t(0.7735,0.5368)\n",
      "43\t0.7613\t0.7595\t0.5700\t(0.7735,0.5368)\n",
      "44\t0.7810\t0.7756\t0.5603\t(0.7810,0.5603)\n",
      "45\t0.7673\t0.7686\t0.5575\t(0.7810,0.5603)\n",
      "46\t0.7787\t0.7773\t0.5560\t(0.7810,0.5603)\n",
      "47\t0.7708\t0.7684\t0.5427\t(0.7810,0.5603)\n",
      "48\t0.7701\t0.7671\t0.5677\t(0.7810,0.5603)\n",
      "49\t0.7746\t0.7722\t0.5616\t(0.7810,0.5603)\n",
      "50\t0.7834\t0.7781\t0.5714\t(0.7834,0.5714)\n",
      "51\t0.7560\t0.7562\t0.5684\t(0.7834,0.5714)\n",
      "52\t0.7902\t0.7812\t0.5713\t(0.7902,0.5713)\n",
      "53\t0.7719\t0.7637\t0.5707\t(0.7902,0.5713)\n",
      "54\t0.7797\t0.7722\t0.5672\t(0.7902,0.5713)\n",
      "55\t0.7875\t0.7774\t0.5730\t(0.7902,0.5713)\n",
      "56\t0.7909\t0.7851\t0.5725\t(0.7909,0.5725)\n",
      "57\t0.7699\t0.7629\t0.5695\t(0.7909,0.5725)\n",
      "58\t0.7628\t0.7665\t0.5656\t(0.7909,0.5725)\n",
      "59\t0.7774\t0.7792\t0.5536\t(0.7909,0.5725)\n",
      "60\t0.7906\t0.7870\t0.5775\t(0.7909,0.5725)\n",
      "61\t0.7847\t0.7796\t0.5692\t(0.7909,0.5725)\n",
      "62\t0.7871\t0.7807\t0.5671\t(0.7909,0.5725)\n",
      "63\t0.7845\t0.7821\t0.5703\t(0.7909,0.5725)\n",
      "64\t0.7899\t0.7861\t0.5663\t(0.7909,0.5725)\n",
      "65\t0.7974\t0.7968\t0.5737\t(0.7974,0.5737)\n",
      "66\t0.7929\t0.7956\t0.5604\t(0.7974,0.5737)\n",
      "67\t0.7937\t0.7798\t0.5731\t(0.7974,0.5737)\n",
      "68\t0.7813\t0.7786\t0.5773\t(0.7974,0.5737)\n",
      "69\t0.8004\t0.7916\t0.5754\t(0.8004,0.5754)\n",
      "70\t0.7914\t0.7877\t0.5613\t(0.8004,0.5754)\n",
      "71\t0.7931\t0.7931\t0.5623\t(0.8004,0.5754)\n",
      "72\t0.7951\t0.7900\t0.5764\t(0.8004,0.5754)\n",
      "73\t0.7954\t0.7921\t0.5708\t(0.8004,0.5754)\n",
      "74\t0.7871\t0.7844\t0.5639\t(0.8004,0.5754)\n",
      "75\t0.7913\t0.7910\t0.5675\t(0.8004,0.5754)\n",
      "76\t0.7939\t0.7947\t0.5624\t(0.8004,0.5754)\n",
      "77\t0.7973\t0.7913\t0.5555\t(0.8004,0.5754)\n",
      "78\t0.7989\t0.7988\t0.5743\t(0.8004,0.5754)\n",
      "79\t0.7933\t0.7965\t0.5625\t(0.8004,0.5754)\n",
      "80\t0.7898\t0.7909\t0.5714\t(0.8004,0.5754)\n",
      "81\t0.7976\t0.7941\t0.5552\t(0.8004,0.5754)\n",
      "82\t0.7958\t0.7952\t0.5677\t(0.8004,0.5754)\n",
      "83\t0.7930\t0.7993\t0.5537\t(0.8004,0.5754)\n",
      "84\t0.7963\t0.7961\t0.5575\t(0.8004,0.5754)\n",
      "85\t0.7993\t0.8030\t0.5560\t(0.8004,0.5754)\n",
      "86\t0.7967\t0.8029\t0.5769\t(0.8004,0.5754)\n",
      "87\t0.7987\t0.8045\t0.5740\t(0.8004,0.5754)\n",
      "88\t0.7956\t0.7931\t0.5649\t(0.8004,0.5754)\n",
      "89\t0.8022\t0.7979\t0.5499\t(0.8022,0.5499)\n",
      "90\t0.8058\t0.8031\t0.5572\t(0.8058,0.5572)\n",
      "91\t0.8019\t0.8009\t0.5627\t(0.8058,0.5572)\n",
      "92\t0.8023\t0.7985\t0.5599\t(0.8058,0.5572)\n",
      "93\t0.8057\t0.8036\t0.5633\t(0.8058,0.5572)\n",
      "94\t0.8070\t0.8079\t0.5587\t(0.8070,0.5587)\n",
      "95\t0.8089\t0.8083\t0.5545\t(0.8089,0.5545)\n",
      "96\t0.8062\t0.8054\t0.5546\t(0.8089,0.5545)\n",
      "97\t0.8087\t0.8025\t0.5652\t(0.8089,0.5545)\n",
      "98\t0.8132\t0.8034\t0.5490\t(0.8132,0.5490)\n",
      "99\t0.8037\t0.8011\t0.5550\t(0.8132,0.5490)\n",
      "100\t0.8006\t0.7984\t0.5608\t(0.8132,0.5490)\n",
      "101\t0.8083\t0.7992\t0.5614\t(0.8132,0.5490)\n",
      "102\t0.8074\t0.8050\t0.5492\t(0.8132,0.5490)\n",
      "103\t0.8091\t0.8024\t0.5518\t(0.8132,0.5490)\n",
      "104\t0.8074\t0.8048\t0.5504\t(0.8132,0.5490)\n",
      "105\t0.8095\t0.8008\t0.5511\t(0.8132,0.5490)\n",
      "106\t0.8090\t0.7959\t0.5520\t(0.8132,0.5490)\n",
      "107\t0.8135\t0.8105\t0.5594\t(0.8135,0.5594)\n",
      "108\t0.8179\t0.8059\t0.5534\t(0.8179,0.5534)\n",
      "109\t0.8178\t0.8104\t0.5584\t(0.8179,0.5534)\n",
      "110\t0.8142\t0.8053\t0.5557\t(0.8179,0.5534)\n",
      "111\t0.8130\t0.8067\t0.5505\t(0.8179,0.5534)\n",
      "112\t0.8194\t0.8070\t0.5565\t(0.8194,0.5565)\n",
      "113\t0.8158\t0.8077\t0.5558\t(0.8194,0.5565)\n",
      "114\t0.8112\t0.8079\t0.5581\t(0.8194,0.5565)\n",
      "115\t0.8128\t0.8064\t0.5588\t(0.8194,0.5565)\n",
      "116\t0.8150\t0.8089\t0.5644\t(0.8194,0.5565)\n",
      "117\t0.8184\t0.8105\t0.5466\t(0.8194,0.5565)\n",
      "118\t0.8183\t0.8101\t0.5579\t(0.8194,0.5565)\n",
      "119\t0.8151\t0.8134\t0.5511\t(0.8194,0.5565)\n",
      "120\t0.8172\t0.8098\t0.5535\t(0.8194,0.5565)\n",
      "121\t0.8178\t0.8095\t0.5518\t(0.8194,0.5565)\n",
      "122\t0.8141\t0.8103\t0.5631\t(0.8194,0.5565)\n",
      "123\t0.8188\t0.8132\t0.5613\t(0.8194,0.5565)\n",
      "124\t0.8172\t0.8138\t0.5657\t(0.8194,0.5565)\n",
      "125\t0.8189\t0.8126\t0.5661\t(0.8194,0.5565)\n",
      "126\t0.8184\t0.8127\t0.5593\t(0.8194,0.5565)\n",
      "127\t0.8191\t0.8155\t0.5583\t(0.8194,0.5565)\n",
      "128\t0.8202\t0.8102\t0.5577\t(0.8202,0.5577)\n",
      "129\t0.8214\t0.8090\t0.5588\t(0.8214,0.5588)\n",
      "130\t0.8225\t0.8106\t0.5638\t(0.8225,0.5638)\n",
      "131\t0.8186\t0.8123\t0.5538\t(0.8225,0.5638)\n",
      "132\t0.8186\t0.8146\t0.5522\t(0.8225,0.5638)\n",
      "133\t0.8212\t0.8122\t0.5602\t(0.8225,0.5638)\n",
      "134\t0.8244\t0.8119\t0.5559\t(0.8244,0.5559)\n",
      "135\t0.8224\t0.8128\t0.5540\t(0.8244,0.5559)\n",
      "136\t0.8215\t0.8125\t0.5615\t(0.8244,0.5559)\n",
      "137\t0.8220\t0.8136\t0.5612\t(0.8244,0.5559)\n",
      "138\t0.8232\t0.8122\t0.5556\t(0.8244,0.5559)\n",
      "139\t0.8211\t0.8083\t0.5562\t(0.8244,0.5559)\n",
      "140\t0.8243\t0.8186\t0.5556\t(0.8244,0.5559)\n",
      "141\t0.8238\t0.8134\t0.5557\t(0.8244,0.5559)\n",
      "142\t0.8234\t0.8139\t0.5568\t(0.8244,0.5559)\n",
      "143\t0.8231\t0.8148\t0.5606\t(0.8244,0.5559)\n",
      "144\t0.8233\t0.8126\t0.5567\t(0.8244,0.5559)\n",
      "145\t0.8217\t0.8154\t0.5543\t(0.8244,0.5559)\n",
      "146\t0.8218\t0.8136\t0.5610\t(0.8244,0.5559)\n",
      "147\t0.8228\t0.8150\t0.5555\t(0.8244,0.5559)\n",
      "148\t0.8219\t0.8195\t0.5525\t(0.8244,0.5559)\n",
      "149\t0.8214\t0.8151\t0.5538\t(0.8244,0.5559)\n",
      "\n",
      "ablation: False\n",
      "ablation: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tnan\tnan\tnan\t(-inf,-inf)\n",
      "1\t0.5568\t0.5379\t0.6692\t(0.5568,0.6692)\n",
      "2\t0.6198\t0.6160\t0.6348\t(0.6198,0.6348)\n",
      "3\t0.5700\t0.5543\t0.6028\t(0.6198,0.6348)\n",
      "4\t0.6771\t0.6693\t0.5902\t(0.6771,0.5902)\n",
      "5\t0.7032\t0.7012\t0.5530\t(0.7032,0.5530)\n",
      "6\t0.7040\t0.6943\t0.5479\t(0.7040,0.5479)\n",
      "7\t0.7322\t0.7266\t0.5185\t(0.7322,0.5185)\n",
      "8\t0.7240\t0.7196\t0.5158\t(0.7322,0.5185)\n",
      "9\t0.7327\t0.7291\t0.5131\t(0.7327,0.5131)\n",
      "10\t0.7228\t0.7206\t0.5122\t(0.7327,0.5131)\n",
      "11\t0.7476\t0.7382\t0.5037\t(0.7476,0.5037)\n",
      "12\t0.7489\t0.7434\t0.4898\t(0.7489,0.4898)\n",
      "13\t0.7379\t0.7369\t0.5133\t(0.7489,0.4898)\n",
      "14\t0.7226\t0.7150\t0.5004\t(0.7489,0.4898)\n",
      "15\t0.7565\t0.7488\t0.4929\t(0.7565,0.4929)\n",
      "16\t0.7407\t0.7366\t0.5089\t(0.7565,0.4929)\n",
      "17\t0.7618\t0.7563\t0.4947\t(0.7618,0.4947)\n",
      "18\t0.7610\t0.7607\t0.5075\t(0.7618,0.4947)\n",
      "19\t0.7488\t0.7433\t0.5165\t(0.7618,0.4947)\n",
      "20\t0.7612\t0.7579\t0.5083\t(0.7618,0.4947)\n",
      "21\t0.7638\t0.7641\t0.5087\t(0.7638,0.5087)\n",
      "22\t0.7589\t0.7559\t0.5134\t(0.7638,0.5087)\n",
      "23\t0.7568\t0.7574\t0.5101\t(0.7638,0.5087)\n",
      "24\t0.7649\t0.7694\t0.5119\t(0.7649,0.5119)\n",
      "25\t0.7539\t0.7598\t0.5195\t(0.7649,0.5119)\n",
      "26\t0.7749\t0.7676\t0.5140\t(0.7749,0.5140)\n",
      "27\t0.7723\t0.7640\t0.5185\t(0.7749,0.5140)\n",
      "28\t0.7686\t0.7712\t0.5233\t(0.7749,0.5140)\n",
      "29\t0.7611\t0.7613\t0.5074\t(0.7749,0.5140)\n",
      "30\t0.7655\t0.7656\t0.5285\t(0.7749,0.5140)\n",
      "31\t0.7691\t0.7655\t0.5177\t(0.7749,0.5140)\n",
      "32\t0.7757\t0.7693\t0.5129\t(0.7757,0.5129)\n",
      "33\t0.7732\t0.7694\t0.5168\t(0.7757,0.5129)\n",
      "34\t0.7781\t0.7693\t0.5080\t(0.7781,0.5080)\n",
      "35\t0.7729\t0.7689\t0.5157\t(0.7781,0.5080)\n",
      "36\t0.7717\t0.7806\t0.5148\t(0.7781,0.5080)\n",
      "37\t0.7743\t0.7773\t0.5288\t(0.7781,0.5080)\n",
      "38\t0.7734\t0.7755\t0.5267\t(0.7781,0.5080)\n",
      "39\t0.7751\t0.7740\t0.5233\t(0.7781,0.5080)\n",
      "40\t0.7797\t0.7817\t0.5208\t(0.7797,0.5208)\n",
      "41\t0.7782\t0.7780\t0.5183\t(0.7797,0.5208)\n",
      "42\t0.7824\t0.7820\t0.5146\t(0.7824,0.5146)\n",
      "43\t0.7784\t0.7780\t0.5163\t(0.7824,0.5146)\n",
      "44\t0.7778\t0.7794\t0.5363\t(0.7824,0.5146)\n",
      "45\t0.7706\t0.7748\t0.5187\t(0.7824,0.5146)\n",
      "46\t0.7790\t0.7852\t0.5179\t(0.7824,0.5146)\n",
      "47\t0.7742\t0.7703\t0.5147\t(0.7824,0.5146)\n",
      "48\t0.7829\t0.7862\t0.5220\t(0.7829,0.5220)\n",
      "49\t0.7842\t0.7783\t0.5156\t(0.7842,0.5156)\n",
      "50\t0.7829\t0.7810\t0.5115\t(0.7842,0.5156)\n",
      "51\t0.7821\t0.7829\t0.5137\t(0.7842,0.5156)\n",
      "52\t0.7773\t0.7812\t0.5213\t(0.7842,0.5156)\n",
      "53\t0.7844\t0.7863\t0.5169\t(0.7844,0.5169)\n",
      "54\t0.7869\t0.7820\t0.5157\t(0.7869,0.5157)\n",
      "55\t0.7874\t0.7842\t0.5131\t(0.7874,0.5131)\n",
      "56\t0.7861\t0.7860\t0.5207\t(0.7874,0.5131)\n",
      "57\t0.7894\t0.7848\t0.5185\t(0.7894,0.5185)\n",
      "58\t0.7856\t0.7812\t0.5295\t(0.7894,0.5185)\n",
      "59\t0.7863\t0.7826\t0.5196\t(0.7894,0.5185)\n",
      "60\t0.7907\t0.7905\t0.5270\t(0.7907,0.5270)\n",
      "61\t0.7854\t0.7826\t0.5225\t(0.7907,0.5270)\n",
      "62\t0.7829\t0.7853\t0.5308\t(0.7907,0.5270)\n",
      "63\t0.7877\t0.7886\t0.5270\t(0.7907,0.5270)\n",
      "64\t0.7863\t0.7865\t0.5276\t(0.7907,0.5270)\n",
      "65\t0.7886\t0.7867\t0.5203\t(0.7907,0.5270)\n",
      "66\t0.7958\t0.7935\t0.5192\t(0.7958,0.5192)\n",
      "67\t0.7935\t0.7882\t0.5278\t(0.7958,0.5192)\n",
      "68\t0.7901\t0.7920\t0.5233\t(0.7958,0.5192)\n",
      "69\t0.7892\t0.7863\t0.5237\t(0.7958,0.5192)\n",
      "70\t0.7925\t0.7896\t0.5131\t(0.7958,0.5192)\n",
      "71\t0.7940\t0.7924\t0.5241\t(0.7958,0.5192)\n",
      "72\t0.7959\t0.7896\t0.5176\t(0.7959,0.5176)\n",
      "73\t0.7940\t0.7890\t0.5059\t(0.7959,0.5176)\n",
      "74\t0.7946\t0.7884\t0.5058\t(0.7959,0.5176)\n",
      "75\t0.7924\t0.7855\t0.5111\t(0.7959,0.5176)\n",
      "76\t0.7983\t0.7938\t0.5245\t(0.7983,0.5245)\n",
      "77\t0.7971\t0.7892\t0.5141\t(0.7983,0.5245)\n",
      "78\t0.7959\t0.7842\t0.5191\t(0.7983,0.5245)\n",
      "79\t0.7930\t0.7846\t0.5040\t(0.7983,0.5245)\n",
      "80\t0.7965\t0.7927\t0.5226\t(0.7983,0.5245)\n",
      "81\t0.7904\t0.7905\t0.5065\t(0.7983,0.5245)\n",
      "82\t0.8005\t0.7933\t0.5064\t(0.8005,0.5064)\n",
      "83\t0.7996\t0.7925\t0.5275\t(0.8005,0.5064)\n",
      "84\t0.7944\t0.7865\t0.5201\t(0.8005,0.5064)\n",
      "85\t0.8001\t0.7971\t0.5148\t(0.8005,0.5064)\n",
      "86\t0.7860\t0.7838\t0.5094\t(0.8005,0.5064)\n",
      "87\t0.8062\t0.7924\t0.5219\t(0.8062,0.5219)\n",
      "88\t0.7994\t0.7941\t0.5083\t(0.8062,0.5219)\n",
      "89\t0.7957\t0.7879\t0.5110\t(0.8062,0.5219)\n",
      "90\t0.8054\t0.8010\t0.5222\t(0.8062,0.5219)\n",
      "91\t0.7953\t0.7906\t0.5301\t(0.8062,0.5219)\n",
      "92\t0.8020\t0.7966\t0.5098\t(0.8062,0.5219)\n",
      "93\t0.7998\t0.7982\t0.5242\t(0.8062,0.5219)\n",
      "94\t0.7915\t0.7883\t0.5152\t(0.8062,0.5219)\n",
      "95\t0.7972\t0.7937\t0.5187\t(0.8062,0.5219)\n",
      "96\t0.8035\t0.7934\t0.5147\t(0.8062,0.5219)\n",
      "97\t0.7958\t0.7900\t0.5220\t(0.8062,0.5219)\n",
      "98\t0.8056\t0.7998\t0.5152\t(0.8062,0.5219)\n",
      "99\t0.7997\t0.7932\t0.5139\t(0.8062,0.5219)\n",
      "100\t0.7989\t0.7939\t0.5247\t(0.8062,0.5219)\n",
      "101\t0.8055\t0.7974\t0.5180\t(0.8062,0.5219)\n",
      "102\t0.8006\t0.7899\t0.5147\t(0.8062,0.5219)\n",
      "103\t0.7981\t0.7906\t0.5157\t(0.8062,0.5219)\n",
      "104\t0.8012\t0.7922\t0.5131\t(0.8062,0.5219)\n",
      "105\t0.7960\t0.7880\t0.5282\t(0.8062,0.5219)\n",
      "106\t0.8052\t0.7980\t0.5203\t(0.8062,0.5219)\n",
      "107\t0.8047\t0.7946\t0.5160\t(0.8062,0.5219)\n",
      "108\t0.8008\t0.7898\t0.5225\t(0.8062,0.5219)\n",
      "109\t0.8005\t0.7936\t0.5043\t(0.8062,0.5219)\n",
      "110\t0.7994\t0.7968\t0.5262\t(0.8062,0.5219)\n",
      "111\t0.8070\t0.8037\t0.5242\t(0.8070,0.5242)\n",
      "112\t0.8075\t0.8033\t0.5305\t(0.8075,0.5305)\n",
      "113\t0.8112\t0.8022\t0.5161\t(0.8112,0.5161)\n",
      "114\t0.8075\t0.8034\t0.5088\t(0.8112,0.5161)\n",
      "115\t0.8065\t0.7938\t0.5088\t(0.8112,0.5161)\n",
      "116\t0.8080\t0.8023\t0.5110\t(0.8112,0.5161)\n",
      "117\t0.8107\t0.8015\t0.5345\t(0.8112,0.5161)\n",
      "118\t0.8078\t0.8026\t0.5244\t(0.8112,0.5161)\n",
      "119\t0.8075\t0.8036\t0.5283\t(0.8112,0.5161)\n",
      "120\t0.8149\t0.8023\t0.5044\t(0.8149,0.5044)\n",
      "121\t0.8080\t0.7995\t0.5214\t(0.8149,0.5044)\n",
      "122\t0.8135\t0.7996\t0.5142\t(0.8149,0.5044)\n",
      "123\t0.8082\t0.8056\t0.5090\t(0.8149,0.5044)\n",
      "124\t0.8100\t0.8019\t0.5190\t(0.8149,0.5044)\n",
      "125\t0.8099\t0.7952\t0.5331\t(0.8149,0.5044)\n",
      "126\t0.8111\t0.8036\t0.5182\t(0.8149,0.5044)\n",
      "127\t0.8115\t0.8018\t0.5226\t(0.8149,0.5044)\n",
      "128\t0.8121\t0.8075\t0.5237\t(0.8149,0.5044)\n",
      "129\t0.8078\t0.8005\t0.5222\t(0.8149,0.5044)\n",
      "130\t0.8123\t0.8041\t0.5131\t(0.8149,0.5044)\n",
      "131\t0.8144\t0.8056\t0.5139\t(0.8149,0.5044)\n",
      "132\t0.8192\t0.8077\t0.5268\t(0.8192,0.5268)\n",
      "133\t0.8166\t0.8083\t0.5249\t(0.8192,0.5268)\n",
      "134\t0.8139\t0.8078\t0.5125\t(0.8192,0.5268)\n",
      "135\t0.8170\t0.8108\t0.5155\t(0.8192,0.5268)\n",
      "136\t0.8186\t0.8078\t0.5185\t(0.8192,0.5268)\n",
      "137\t0.8144\t0.8065\t0.5210\t(0.8192,0.5268)\n",
      "138\t0.8159\t0.8049\t0.5235\t(0.8192,0.5268)\n",
      "139\t0.8103\t0.8051\t0.5230\t(0.8192,0.5268)\n",
      "140\t0.8159\t0.8102\t0.5125\t(0.8192,0.5268)\n",
      "141\t0.8182\t0.8103\t0.5165\t(0.8192,0.5268)\n",
      "142\t0.8208\t0.8116\t0.5209\t(0.8208,0.5209)\n",
      "143\t0.8194\t0.8107\t0.5127\t(0.8208,0.5209)\n",
      "144\t0.8146\t0.8084\t0.5269\t(0.8208,0.5209)\n",
      "145\t0.8148\t0.8079\t0.5232\t(0.8208,0.5209)\n",
      "146\t0.8182\t0.8131\t0.5204\t(0.8208,0.5209)\n",
      "147\t0.8187\t0.8108\t0.5137\t(0.8208,0.5209)\n",
      "148\t0.8184\t0.8107\t0.5383\t(0.8208,0.5209)\n",
      "149\t0.8206\t0.8090\t0.5267\t(0.8208,0.5209)\n",
      "\n",
      "ablation: False\n",
      "ablation: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tnan\tnan\tnan\t(-inf,-inf)\n",
      "1\t0.5099\t0.4910\t0.6062\t(0.5099,0.6062)\n",
      "2\t0.6140\t0.6071\t0.6524\t(0.6140,0.6524)\n",
      "3\t0.6541\t0.6534\t0.6353\t(0.6541,0.6353)\n",
      "4\t0.6565\t0.6618\t0.6065\t(0.6565,0.6065)\n",
      "5\t0.6743\t0.6807\t0.5975\t(0.6743,0.5975)\n",
      "6\t0.6916\t0.7003\t0.5805\t(0.6916,0.5805)\n",
      "7\t0.6839\t0.6897\t0.5915\t(0.6916,0.5805)\n",
      "8\t0.7150\t0.7240\t0.5521\t(0.7150,0.5521)\n",
      "9\t0.7120\t0.7258\t0.5496\t(0.7150,0.5521)\n",
      "10\t0.7002\t0.7126\t0.5563\t(0.7150,0.5521)\n",
      "11\t0.7280\t0.7414\t0.5490\t(0.7280,0.5490)\n",
      "12\t0.7239\t0.7258\t0.5612\t(0.7280,0.5490)\n",
      "13\t0.7407\t0.7356\t0.5434\t(0.7407,0.5434)\n",
      "14\t0.7414\t0.7486\t0.5470\t(0.7414,0.5470)\n",
      "15\t0.7211\t0.7224\t0.5571\t(0.7414,0.5470)\n",
      "16\t0.7411\t0.7457\t0.5560\t(0.7414,0.5470)\n",
      "17\t0.7143\t0.7191\t0.5713\t(0.7414,0.5470)\n",
      "18\t0.7393\t0.7499\t0.5715\t(0.7414,0.5470)\n",
      "19\t0.7443\t0.7493\t0.5660\t(0.7443,0.5660)\n",
      "20\t0.7444\t0.7527\t0.5610\t(0.7444,0.5610)\n",
      "21\t0.7303\t0.7326\t0.5774\t(0.7444,0.5610)\n",
      "22\t0.7412\t0.7457\t0.5803\t(0.7444,0.5610)\n",
      "23\t0.7556\t0.7468\t0.5809\t(0.7556,0.5809)\n",
      "24\t0.7614\t0.7648\t0.5671\t(0.7614,0.5671)\n",
      "25\t0.7553\t0.7653\t0.5876\t(0.7614,0.5671)\n",
      "26\t0.7566\t0.7648\t0.5747\t(0.7614,0.5671)\n",
      "27\t0.7620\t0.7634\t0.5845\t(0.7620,0.5845)\n",
      "28\t0.7651\t0.7675\t0.5781\t(0.7651,0.5781)\n",
      "29\t0.7643\t0.7705\t0.5744\t(0.7651,0.5781)\n",
      "30\t0.7615\t0.7672\t0.5864\t(0.7651,0.5781)\n",
      "31\t0.7648\t0.7702\t0.5684\t(0.7651,0.5781)\n",
      "32\t0.7691\t0.7760\t0.5625\t(0.7691,0.5625)\n",
      "33\t0.7718\t0.7763\t0.5654\t(0.7718,0.5654)\n",
      "34\t0.7625\t0.7749\t0.5763\t(0.7718,0.5654)\n",
      "35\t0.7728\t0.7734\t0.5673\t(0.7728,0.5673)\n",
      "36\t0.7734\t0.7720\t0.5785\t(0.7734,0.5785)\n",
      "37\t0.7734\t0.7739\t0.5821\t(0.7734,0.5785)\n",
      "38\t0.7724\t0.7729\t0.5755\t(0.7734,0.5785)\n",
      "39\t0.7752\t0.7794\t0.5805\t(0.7752,0.5805)\n",
      "40\t0.7756\t0.7758\t0.5813\t(0.7756,0.5813)\n",
      "41\t0.7759\t0.7819\t0.5912\t(0.7759,0.5912)\n",
      "42\t0.7741\t0.7748\t0.5951\t(0.7759,0.5912)\n",
      "43\t0.7755\t0.7791\t0.5834\t(0.7759,0.5912)\n",
      "44\t0.7790\t0.7805\t0.6068\t(0.7790,0.6068)\n",
      "45\t0.7742\t0.7847\t0.5831\t(0.7790,0.6068)\n",
      "46\t0.7799\t0.7825\t0.5729\t(0.7799,0.5729)\n",
      "47\t0.7781\t0.7779\t0.5900\t(0.7799,0.5729)\n",
      "48\t0.7795\t0.7803\t0.5840\t(0.7799,0.5729)\n",
      "49\t0.7848\t0.7820\t0.5796\t(0.7848,0.5796)\n",
      "50\t0.7772\t0.7837\t0.5738\t(0.7848,0.5796)\n",
      "51\t0.7832\t0.7864\t0.5837\t(0.7848,0.5796)\n",
      "52\t0.7822\t0.7792\t0.5732\t(0.7848,0.5796)\n",
      "53\t0.7742\t0.7783\t0.5896\t(0.7848,0.5796)\n",
      "54\t0.7761\t0.7794\t0.5947\t(0.7848,0.5796)\n",
      "55\t0.7799\t0.7819\t0.5902\t(0.7848,0.5796)\n",
      "56\t0.7830\t0.7861\t0.5909\t(0.7848,0.5796)\n",
      "57\t0.7793\t0.7858\t0.5756\t(0.7848,0.5796)\n",
      "58\t0.7826\t0.7917\t0.5735\t(0.7848,0.5796)\n",
      "59\t0.7865\t0.7866\t0.5741\t(0.7865,0.5741)\n",
      "60\t0.7863\t0.7868\t0.5906\t(0.7865,0.5741)\n",
      "61\t0.7856\t0.7898\t0.5795\t(0.7865,0.5741)\n",
      "62\t0.7935\t0.7980\t0.5715\t(0.7935,0.5715)\n",
      "63\t0.7915\t0.7932\t0.5788\t(0.7935,0.5715)\n",
      "64\t0.7946\t0.7995\t0.5716\t(0.7946,0.5716)\n",
      "65\t0.7951\t0.7966\t0.5742\t(0.7951,0.5742)\n",
      "66\t0.7895\t0.7933\t0.5696\t(0.7951,0.5742)\n",
      "67\t0.7921\t0.7919\t0.5730\t(0.7951,0.5742)\n",
      "68\t0.7925\t0.7934\t0.5768\t(0.7951,0.5742)\n",
      "69\t0.7907\t0.7906\t0.5739\t(0.7951,0.5742)\n",
      "70\t0.7939\t0.7958\t0.5765\t(0.7951,0.5742)\n",
      "71\t0.7905\t0.7933\t0.5825\t(0.7951,0.5742)\n",
      "72\t0.7847\t0.7887\t0.5794\t(0.7951,0.5742)\n",
      "73\t0.7882\t0.7916\t0.5716\t(0.7951,0.5742)\n",
      "74\t0.7918\t0.7960\t0.5740\t(0.7951,0.5742)\n",
      "75\t0.7964\t0.7971\t0.5925\t(0.7964,0.5925)\n",
      "76\t0.7890\t0.7934\t0.5810\t(0.7964,0.5925)\n",
      "77\t0.7955\t0.8018\t0.5864\t(0.7964,0.5925)\n",
      "78\t0.7883\t0.7940\t0.5700\t(0.7964,0.5925)\n",
      "79\t0.7929\t0.7931\t0.5801\t(0.7964,0.5925)\n",
      "80\t0.7973\t0.7998\t0.5806\t(0.7973,0.5806)\n",
      "81\t0.7961\t0.7974\t0.5828\t(0.7973,0.5806)\n",
      "82\t0.7978\t0.7952\t0.5837\t(0.7978,0.5837)\n",
      "83\t0.8005\t0.8046\t0.5808\t(0.8005,0.5808)\n",
      "84\t0.7936\t0.7980\t0.5774\t(0.8005,0.5808)\n",
      "85\t0.7973\t0.7923\t0.5787\t(0.8005,0.5808)\n",
      "86\t0.7952\t0.7964\t0.5740\t(0.8005,0.5808)\n",
      "87\t0.7916\t0.7962\t0.5740\t(0.8005,0.5808)\n",
      "88\t0.8019\t0.8019\t0.5835\t(0.8019,0.5835)\n",
      "89\t0.7931\t0.7999\t0.5818\t(0.8019,0.5835)\n",
      "90\t0.7990\t0.8028\t0.5770\t(0.8019,0.5835)\n",
      "91\t0.7981\t0.8032\t0.5929\t(0.8019,0.5835)\n",
      "92\t0.8031\t0.7983\t0.5861\t(0.8031,0.5861)\n",
      "93\t0.8013\t0.8013\t0.5860\t(0.8031,0.5861)\n",
      "94\t0.7952\t0.7991\t0.5813\t(0.8031,0.5861)\n",
      "95\t0.7983\t0.8031\t0.5794\t(0.8031,0.5861)\n",
      "96\t0.8030\t0.8110\t0.5834\t(0.8031,0.5861)\n",
      "97\t0.8026\t0.8031\t0.5877\t(0.8031,0.5861)\n",
      "98\t0.8035\t0.8046\t0.5860\t(0.8035,0.5860)\n",
      "99\t0.8083\t0.8048\t0.5942\t(0.8083,0.5942)\n",
      "100\t0.8012\t0.7992\t0.5905\t(0.8083,0.5942)\n",
      "101\t0.8052\t0.8016\t0.5927\t(0.8083,0.5942)\n",
      "102\t0.7981\t0.7982\t0.5865\t(0.8083,0.5942)\n",
      "103\t0.7961\t0.7917\t0.5844\t(0.8083,0.5942)\n",
      "104\t0.7993\t0.7991\t0.5872\t(0.8083,0.5942)\n",
      "105\t0.8008\t0.8000\t0.5905\t(0.8083,0.5942)\n",
      "106\t0.8076\t0.8045\t0.5930\t(0.8083,0.5942)\n",
      "107\t0.8039\t0.8023\t0.5861\t(0.8083,0.5942)\n",
      "108\t0.8059\t0.8003\t0.5986\t(0.8083,0.5942)\n",
      "109\t0.8017\t0.8032\t0.5884\t(0.8083,0.5942)\n",
      "110\t0.8073\t0.8087\t0.5958\t(0.8083,0.5942)\n",
      "111\t0.8057\t0.8096\t0.5953\t(0.8083,0.5942)\n",
      "112\t0.8131\t0.8097\t0.6059\t(0.8131,0.6059)\n",
      "113\t0.8142\t0.8098\t0.6030\t(0.8142,0.6030)\n",
      "114\t0.8113\t0.8082\t0.5925\t(0.8142,0.6030)\n",
      "115\t0.8134\t0.8151\t0.6042\t(0.8142,0.6030)\n",
      "116\t0.8105\t0.8054\t0.6000\t(0.8142,0.6030)\n",
      "117\t0.8129\t0.8101\t0.5925\t(0.8142,0.6030)\n",
      "118\t0.8123\t0.8110\t0.6033\t(0.8142,0.6030)\n",
      "119\t0.8108\t0.8104\t0.5941\t(0.8142,0.6030)\n",
      "120\t0.8125\t0.8143\t0.5962\t(0.8142,0.6030)\n",
      "121\t0.8080\t0.8071\t0.6004\t(0.8142,0.6030)\n",
      "122\t0.8111\t0.8130\t0.5994\t(0.8142,0.6030)\n",
      "123\t0.8125\t0.8071\t0.5966\t(0.8142,0.6030)\n",
      "124\t0.8109\t0.8078\t0.5951\t(0.8142,0.6030)\n",
      "125\t0.8115\t0.8123\t0.5998\t(0.8142,0.6030)\n",
      "126\t0.8125\t0.8102\t0.5921\t(0.8142,0.6030)\n",
      "127\t0.8129\t0.8101\t0.6019\t(0.8142,0.6030)\n",
      "128\t0.8100\t0.8091\t0.5936\t(0.8142,0.6030)\n",
      "129\t0.8147\t0.8135\t0.5940\t(0.8147,0.5940)\n",
      "130\t0.8151\t0.8131\t0.5965\t(0.8151,0.5965)\n",
      "131\t0.8149\t0.8145\t0.5965\t(0.8151,0.5965)\n",
      "132\t0.8148\t0.8142\t0.5974\t(0.8151,0.5965)\n",
      "133\t0.8147\t0.8159\t0.5986\t(0.8151,0.5965)\n",
      "134\t0.8166\t0.8142\t0.5942\t(0.8166,0.5942)\n",
      "135\t0.8165\t0.8123\t0.5947\t(0.8166,0.5942)\n",
      "136\t0.8162\t0.8140\t0.6015\t(0.8166,0.5942)\n",
      "137\t0.8171\t0.8141\t0.5984\t(0.8171,0.5984)\n",
      "138\t0.8176\t0.8148\t0.5954\t(0.8176,0.5954)\n",
      "139\t0.8182\t0.8163\t0.6003\t(0.8182,0.6003)\n",
      "140\t0.8195\t0.8153\t0.5957\t(0.8195,0.5957)\n",
      "141\t0.8176\t0.8159\t0.5937\t(0.8195,0.5957)\n",
      "142\t0.8167\t0.8151\t0.5965\t(0.8195,0.5957)\n",
      "143\t0.8166\t0.8120\t0.5908\t(0.8195,0.5957)\n",
      "144\t0.8180\t0.8158\t0.5971\t(0.8195,0.5957)\n",
      "145\t0.8183\t0.8173\t0.5954\t(0.8195,0.5957)\n",
      "146\t0.8178\t0.8169\t0.5968\t(0.8195,0.5957)\n",
      "147\t0.8161\t0.8152\t0.5954\t(0.8195,0.5957)\n",
      "148\t0.8176\t0.8202\t0.5972\t(0.8195,0.5957)\n",
      "149\t0.8188\t0.8176\t0.5963\t(0.8195,0.5957)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ERM_model_list = []\n",
    "ERM_cv_log = []\n",
    "ERM_model_list, df_ERM_cv_log = cv_train('erm',dataset_TRVL,dataset_TEST_IID,dataset_TEST_OOD,ablation=False,\n",
    "         model_list=ERM_model_list,cv_log=ERM_cv_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c18d927-96c1-4903-896d-713ec22d66ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"7\" halign=\"left\">Model_0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Model_1</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Model_3</th>\n",
       "      <th colspan=\"7\" halign=\"left\">Model_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>val_score</th>\n",
       "      <th>iid_score</th>\n",
       "      <th>ood_score</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>...</th>\n",
       "      <th>iid_score</th>\n",
       "      <th>ood_score</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>val_score</th>\n",
       "      <th>iid_score</th>\n",
       "      <th>ood_score</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1982.007381</td>\n",
       "      <td>1216.681386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0</td>\n",
       "      <td>1941.988552</td>\n",
       "      <td>1164.747638</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0</td>\n",
       "      <td>2012.438048</td>\n",
       "      <td>1168.784162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1172.369864</td>\n",
       "      <td>978.815002</td>\n",
       "      <td>0.536635</td>\n",
       "      <td>0.520937</td>\n",
       "      <td>0.671475</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1</td>\n",
       "      <td>1161.032538</td>\n",
       "      <td>908.661000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537896</td>\n",
       "      <td>0.669210</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1</td>\n",
       "      <td>1169.601975</td>\n",
       "      <td>958.636173</td>\n",
       "      <td>0.509854</td>\n",
       "      <td>0.491030</td>\n",
       "      <td>0.606196</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1006.674239</td>\n",
       "      <td>817.379786</td>\n",
       "      <td>0.606575</td>\n",
       "      <td>0.613854</td>\n",
       "      <td>0.652836</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2</td>\n",
       "      <td>997.469137</td>\n",
       "      <td>768.077525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615971</td>\n",
       "      <td>0.634848</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2</td>\n",
       "      <td>1015.151989</td>\n",
       "      <td>825.510986</td>\n",
       "      <td>0.614026</td>\n",
       "      <td>0.607108</td>\n",
       "      <td>0.652443</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>884.811293</td>\n",
       "      <td>728.081320</td>\n",
       "      <td>0.636993</td>\n",
       "      <td>0.646404</td>\n",
       "      <td>0.633986</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3</td>\n",
       "      <td>891.204818</td>\n",
       "      <td>711.054957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554342</td>\n",
       "      <td>0.602843</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3</td>\n",
       "      <td>907.347135</td>\n",
       "      <td>738.595226</td>\n",
       "      <td>0.654108</td>\n",
       "      <td>0.653402</td>\n",
       "      <td>0.635329</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>837.888278</td>\n",
       "      <td>683.205311</td>\n",
       "      <td>0.630172</td>\n",
       "      <td>0.632385</td>\n",
       "      <td>0.623412</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4</td>\n",
       "      <td>840.905144</td>\n",
       "      <td>697.576520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669277</td>\n",
       "      <td>0.590174</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4</td>\n",
       "      <td>836.350348</td>\n",
       "      <td>703.811875</td>\n",
       "      <td>0.656503</td>\n",
       "      <td>0.661762</td>\n",
       "      <td>0.606507</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>145</td>\n",
       "      <td>359.264946</td>\n",
       "      <td>418.129362</td>\n",
       "      <td>0.803344</td>\n",
       "      <td>0.811849</td>\n",
       "      <td>0.616196</td>\n",
       "      <td>0.002</td>\n",
       "      <td>145</td>\n",
       "      <td>334.304927</td>\n",
       "      <td>371.659484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807911</td>\n",
       "      <td>0.523234</td>\n",
       "      <td>0.002</td>\n",
       "      <td>145</td>\n",
       "      <td>327.344837</td>\n",
       "      <td>368.234632</td>\n",
       "      <td>0.818323</td>\n",
       "      <td>0.817276</td>\n",
       "      <td>0.595380</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>357.075971</td>\n",
       "      <td>423.604783</td>\n",
       "      <td>0.806297</td>\n",
       "      <td>0.815588</td>\n",
       "      <td>0.608114</td>\n",
       "      <td>0.002</td>\n",
       "      <td>146</td>\n",
       "      <td>334.146345</td>\n",
       "      <td>373.104980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813104</td>\n",
       "      <td>0.520418</td>\n",
       "      <td>0.002</td>\n",
       "      <td>146</td>\n",
       "      <td>319.695579</td>\n",
       "      <td>373.385175</td>\n",
       "      <td>0.817833</td>\n",
       "      <td>0.816918</td>\n",
       "      <td>0.596803</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>350.030584</td>\n",
       "      <td>403.444564</td>\n",
       "      <td>0.810628</td>\n",
       "      <td>0.812476</td>\n",
       "      <td>0.619307</td>\n",
       "      <td>0.002</td>\n",
       "      <td>147</td>\n",
       "      <td>339.731209</td>\n",
       "      <td>377.728725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810764</td>\n",
       "      <td>0.513701</td>\n",
       "      <td>0.002</td>\n",
       "      <td>147</td>\n",
       "      <td>328.591776</td>\n",
       "      <td>372.564629</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.815164</td>\n",
       "      <td>0.595449</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>357.384038</td>\n",
       "      <td>399.562432</td>\n",
       "      <td>0.807447</td>\n",
       "      <td>0.815142</td>\n",
       "      <td>0.615818</td>\n",
       "      <td>0.002</td>\n",
       "      <td>148</td>\n",
       "      <td>336.431475</td>\n",
       "      <td>373.615243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810681</td>\n",
       "      <td>0.538339</td>\n",
       "      <td>0.002</td>\n",
       "      <td>148</td>\n",
       "      <td>327.174681</td>\n",
       "      <td>371.118712</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.820156</td>\n",
       "      <td>0.597170</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>361.408128</td>\n",
       "      <td>403.966669</td>\n",
       "      <td>0.805971</td>\n",
       "      <td>0.813230</td>\n",
       "      <td>0.618071</td>\n",
       "      <td>0.002</td>\n",
       "      <td>149</td>\n",
       "      <td>337.958879</td>\n",
       "      <td>372.622487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.526747</td>\n",
       "      <td>0.002</td>\n",
       "      <td>149</td>\n",
       "      <td>322.090268</td>\n",
       "      <td>371.379190</td>\n",
       "      <td>0.818809</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model_0                                                                 \\\n",
       "      epoch   loss_train     loss_val val_score iid_score ood_score     lr   \n",
       "0         0  1982.007381  1216.681386       NaN       NaN       NaN  0.002   \n",
       "1         1  1172.369864   978.815002  0.536635  0.520937  0.671475  0.002   \n",
       "2         2  1006.674239   817.379786  0.606575  0.613854  0.652836  0.002   \n",
       "3         3   884.811293   728.081320  0.636993  0.646404  0.633986  0.002   \n",
       "4         4   837.888278   683.205311  0.630172  0.632385  0.623412  0.002   \n",
       "..      ...          ...          ...       ...       ...       ...    ...   \n",
       "145     145   359.264946   418.129362  0.803344  0.811849  0.616196  0.002   \n",
       "146     146   357.075971   423.604783  0.806297  0.815588  0.608114  0.002   \n",
       "147     147   350.030584   403.444564  0.810628  0.812476  0.619307  0.002   \n",
       "148     148   357.384038   399.562432  0.807447  0.815142  0.615818  0.002   \n",
       "149     149   361.408128   403.966669  0.805971  0.813230  0.618071  0.002   \n",
       "\n",
       "    Model_1                            ...   Model_3                  Model_4  \\\n",
       "      epoch   loss_train     loss_val  ... iid_score ood_score     lr   epoch   \n",
       "0         0  1941.988552  1164.747638  ...       NaN       NaN  0.002       0   \n",
       "1         1  1161.032538   908.661000  ...  0.537896  0.669210  0.002       1   \n",
       "2         2   997.469137   768.077525  ...  0.615971  0.634848  0.002       2   \n",
       "3         3   891.204818   711.054957  ...  0.554342  0.602843  0.002       3   \n",
       "4         4   840.905144   697.576520  ...  0.669277  0.590174  0.002       4   \n",
       "..      ...          ...          ...  ...       ...       ...    ...     ...   \n",
       "145     145   334.304927   371.659484  ...  0.807911  0.523234  0.002     145   \n",
       "146     146   334.146345   373.104980  ...  0.813104  0.520418  0.002     146   \n",
       "147     147   339.731209   377.728725  ...  0.810764  0.513701  0.002     147   \n",
       "148     148   336.431475   373.615243  ...  0.810681  0.538339  0.002     148   \n",
       "149     149   337.958879   372.622487  ...  0.808997  0.526747  0.002     149   \n",
       "\n",
       "                                                                    \n",
       "      loss_train     loss_val val_score iid_score ood_score     lr  \n",
       "0    2012.438048  1168.784162       NaN       NaN       NaN  0.002  \n",
       "1    1169.601975   958.636173  0.509854  0.491030  0.606196  0.002  \n",
       "2    1015.151989   825.510986  0.614026  0.607108  0.652443  0.002  \n",
       "3     907.347135   738.595226  0.654108  0.653402  0.635329  0.002  \n",
       "4     836.350348   703.811875  0.656503  0.661762  0.606507  0.002  \n",
       "..           ...          ...       ...       ...       ...    ...  \n",
       "145   327.344837   368.234632  0.818323  0.817276  0.595380  0.002  \n",
       "146   319.695579   373.385175  0.817833  0.816918  0.596803  0.002  \n",
       "147   328.591776   372.564629  0.816053  0.815164  0.595449  0.002  \n",
       "148   327.174681   371.118712  0.817568  0.820156  0.597170  0.002  \n",
       "149   322.090268   371.379190  0.818809  0.817560  0.596302  0.002  \n",
       "\n",
       "[150 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ERM_cv_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf51372-53b5-445e-992c-a8ec337c0dba",
   "metadata": {},
   "source": [
    "### train_V-REX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f3987-06e2-4956-badf-bf6935efefd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VREX_model_list,VREX_cv_log = cv_train('vrex',dataset_TRVL,dataset_TEST_IID,dataset_TEST_OOD,ablation=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fishr",
   "language": "python",
   "name": "fishr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
